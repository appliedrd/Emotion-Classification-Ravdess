{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjWvnaQUrZmD"
   },
   "source": [
    "# Emotion classification using the RAVDESS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldtHMhuLrewK"
   },
   "source": [
    "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is licensed under CC BY-NA-SC 4.0. and can be downloaded free of charge at https://zenodo.org/record/1188976.\n",
    "\n",
    "***Construction and Validation***\n",
    "\n",
    "Construction and validation of the RAVDESS is described in our paper: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
    "\n",
    "The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLOS ONE.\n",
    "\n",
    "***Description***\n",
    "\n",
    "The dataset contains the complete set of 7356 RAVDESS files (total size: 24.8 GB). Each of the 24 actors consists of three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.\n",
    "\n",
    "***Data***\n",
    "\n",
    "For this task, I have used 4948 samples from the RAVDESS dataset.\n",
    "\n",
    "The samples comes from:\n",
    "\n",
    "- Audio-only files;\n",
    "- Video + audio files: I have extracted the audio from each file using the script Mp4ToWav.py that you can find in the main directory of the project.\n",
    "\n",
    "***License information***\n",
    "\n",
    "The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NA-SC 4.0\n",
    "\n",
    "***File naming convention***\n",
    "\n",
    "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:\n",
    "\n",
    "***Filename identifiers***\n",
    "\n",
    "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "- Vocal channel (01 = speech, 02 = song).\n",
    "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
    "- Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
    "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "Filename example: 02-01-06-01-02-01-12.mp4 \n",
    "\n",
    "- Video-only (02)\n",
    "- Speech (01)\n",
    "- Fearful (06)\n",
    "- Normal intensity (01)\n",
    "- Statement “dogs” (02)\n",
    "- 1st Repetition (01)\n",
    "- 12th Actor (12)\n",
    "- Female, as the actor ID number is even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDNbxj45rkvB"
   },
   "source": [
    "# Analysis\n",
    "\n",
    "We are using Colab, a Google Cloud environment for jupyter, so we need to import our files from Google Drive and then install LibROSA, a python package for music and audio analysis.\n",
    "\n",
    "After the import, we will plot the signal of the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "N-o2JI49WBAe",
    "outputId": "6706e4dc-2ce5-4ab8-ddc5-5cf6093b855f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EgFwaDhMbJVm",
    "outputId": "fc70f467-b88d-44ec-a767-ce29b4467a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /home/ted/anaconda3/lib/python3.6/site-packages (0.6.3)\n",
      "Requirement already satisfied: six>=1.3 in /home/ted/anaconda3/lib/python3.6/site-packages (from librosa) (1.12.0)\n",
      "Requirement already satisfied: numba>=0.38.0 in /home/ted/anaconda3/lib/python3.6/site-packages (from librosa) (0.42.0)\n",
      "Requirement already satisfied: resampy>=0.2.0 in /home/ted/anaconda3/lib/python3.6/site-packages (from librosa) (0.2.1)\n",
      "Requirement already satisfied: joblib>=0.12 in /home/ted/anaconda3/lib/python3.6/site-packages (from librosa) (0.13.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/ted/anaconda3/lib/python3.6/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /home/ted/anaconda3/lib/python3.6/site-packages (from librosa) (2.1.8)\n",
      "Requirement already satisfied: numpy>=1.8.0 in /home/ted/anaconda3/lib/python3.6/site-packages (from librosa) (1.15.4)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /home/ted/anaconda3/lib/python3.6/site-packages (from librosa) (4.3.0)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /home/ted/anaconda3/lib/python3.6/site-packages (from librosa) (0.20.2)\n",
      "Requirement already satisfied: llvmlite>=0.27.0dev0 in /home/ted/anaconda3/lib/python3.6/site-packages (from numba>=0.38.0->librosa) (0.27.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxI4xzngdS-e"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa import display\n",
    "\n",
    "data, sampling_rate = librosa.load('/media/ted/data/git/HHM/Ravdess/Actor_01/03-01-01-01-01-01-01.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "WgaSHtCIdtX2",
    "outputId": "537afffa-9883-46ee-ac49-057c24791853"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x7fe317039be0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% pylab inline\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob \n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCtNuVWlr5jL"
   },
   "source": [
    "# Load all files\n",
    "\n",
    "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AKvuF--gd6F-",
    "outputId": "6b065348-f2b0-4b46-f6c1-24df17681745"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "path = '/media/ted/data/git/HHM/Ravdess/'\n",
    "lst = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoBackendError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4bc0ca31b3ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#Load librosa array, obtain mfcss, store the file and the mcss information in a new array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kaiser_fast'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mmfccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mfcc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# All backends failed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNoBackendError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNoBackendError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for subdir, dirs, files in os.walk(path):\n",
    "  for file in files:\n",
    "      try:\n",
    "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "        file = int(file[7:8]) - 1 \n",
    "        arr = mfccs, file\n",
    "        lst.append(arr)\n",
    "      # If the file is not valid, skip it\n",
    "      except ValueError:\n",
    "        continue\n",
    "\n",
    "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLSggnF7kKY1"
   },
   "outputs": [],
   "source": [
    "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
    "X, y = zip(*lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VzvBRTJIlIE9",
    "outputId": "705b5c38-4b7e-4232-a861-5aa6902e47da"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7acf150849ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOutQiAlCjOY"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0097fdcc017a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msave_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/ted/data/git/HHM/Ravdess/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msavedX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msavedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Saving joblib files to not load them again with the loop above\n",
    "\n",
    "import joblib\n",
    "\n",
    "X_name = 'X.joblib'\n",
    "y_name = 'y.joblib'\n",
    "save_dir = '/media/ted/data/git/HHM/Ravdess/'\n",
    "\n",
    "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
    "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIoFdycUXMxA"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d057928edb13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading saved models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/ted/data/git/HHM/Ravdess/X.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/ted/data/git/HHM/Ravdess/y.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading saved models\n",
    "\n",
    "X = joblib.load('/media/ted/data/git/HHM/Ravdess/X.joblib')\n",
    "y = joblib.load('/media/ted/data/git/HHM/Ravdess/y.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Agw-3KN1sDhh"
   },
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "To make a first attempt in accomplishing this classification task I chose a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-Xgb5NslTBO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UshLOC1ClWL3"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BnCR52nlXw0"
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "qWyTownblZM0",
    "outputId": "7eea5ae9-998e-4fd5-8ee9-87d4c94fc8dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEuw6TUQlr7C"
   },
   "outputs": [],
   "source": [
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1v0i0V7sMw7"
   },
   "source": [
    "Let's go with our classification report.\n",
    "\n",
    "Before we start, a quick reminder of the classes we are trying to predict:\n",
    "\n",
    "emotions = {\n",
    "    \"neutral\": \"0\",\n",
    "    \"calm\": \"1\",\n",
    "    \"happy\": \"2\",\n",
    "    \"sad\": \"3\",\n",
    "    \"angry\": \"4\", \n",
    "    \"fearful\": \"5\", \n",
    "    \"disgust\": \"6\", \n",
    "    \"surprised\": \"7\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "c4kNSYkAleIv",
    "outputId": "e77f2590-34c4-49d2-c889-ce760dc1916b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.83      0.78       134\n",
      "           1       0.87      0.82      0.85       251\n",
      "           2       0.79      0.70      0.74       242\n",
      "           3       0.76      0.77      0.77       271\n",
      "           4       0.85      0.84      0.85       253\n",
      "           5       0.76      0.82      0.79       239\n",
      "           6       0.69      0.71      0.70       127\n",
      "           7       0.72      0.73      0.73       116\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1633\n",
      "   macro avg       0.77      0.78      0.78      1633\n",
      "weighted avg       0.79      0.78      0.78      1633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCVgjLj-gwE2"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jfaTxzZ1w__y"
   },
   "source": [
    "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
    "\n",
    "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcov_DCXgs7v"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eo0ljqzg-KM"
   },
   "outputs": [],
   "source": [
    "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
    "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
    "                                 n_estimators= 22000, random_state= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Tg45qSOfg-26",
    "outputId": "cd802660-45c5-4c5d-fae2-9ca6e597a100"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=20,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=22000, n_jobs=None,\n",
       "            oob_score=False, random_state=5, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aM8KU3qxhGBM"
   },
   "outputs": [],
   "source": [
    "predictions = rforest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "296FW5sBdanI",
    "outputId": "8d7d7c92-2ea2-402c-ae2e-7391e6acc72a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.54      0.70       134\n",
      "           1       0.66      0.96      0.78       251\n",
      "           2       0.86      0.71      0.78       242\n",
      "           3       0.81      0.64      0.71       271\n",
      "           4       0.89      0.88      0.88       253\n",
      "           5       0.70      0.80      0.75       239\n",
      "           6       0.73      0.61      0.66       127\n",
      "           7       0.60      0.78      0.68       116\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1633\n",
      "   macro avg       0.78      0.74      0.74      1633\n",
      "weighted avg       0.79      0.76      0.76      1633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9eqMHV3S8i6"
   },
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-QscoyMxQtn"
   },
   "source": [
    "Let's build our neural network!\n",
    "\n",
    "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4i187-Pe-w5"
   },
   "outputs": [],
   "source": [
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vnvoCRX1gQCh",
    "outputId": "a93a30f0-5d15-4b0b-c46f-62a3bf2e690c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((964, 40, 1), (476, 40, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_traincnn.shape, x_testcnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZOGIpuefCd3"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a614c038aa95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(40,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LphftMIZzUvz"
   },
   "source": [
    "With *model.summary* we can see a recap of what we have build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "pIWPB4Zgfic7",
    "outputId": "aaf3a674-dc56-4bbe-ee67-8c150f471a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qQSBeBhzcLu"
   },
   "source": [
    "Now we can compile and fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNI1znbsfpTx"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34034
    },
    "colab_type": "code",
    "id": "ktdF-nJKfq6F",
    "outputId": "7519d132-59e0-4b35-958f-dd34362b1414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3315 samples, validate on 1633 samples\n",
      "Epoch 1/1000\n",
      "3315/3315 [==============================] - 2s 642us/step - loss: 6.2861 - acc: 0.1508 - val_loss: 2.7693 - val_acc: 0.2107\n",
      "Epoch 2/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 5.1899 - acc: 0.1517 - val_loss: 2.4764 - val_acc: 0.2039\n",
      "Epoch 3/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 4.0180 - acc: 0.1768 - val_loss: 2.1228 - val_acc: 0.2278\n",
      "Epoch 4/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 3.1776 - acc: 0.1928 - val_loss: 1.9904 - val_acc: 0.1935\n",
      "Epoch 5/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 2.5915 - acc: 0.2157 - val_loss: 1.8975 - val_acc: 0.2603\n",
      "Epoch 6/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 2.2226 - acc: 0.2383 - val_loss: 1.8669 - val_acc: 0.2566\n",
      "Epoch 7/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 2.0262 - acc: 0.2486 - val_loss: 1.7460 - val_acc: 0.3301\n",
      "Epoch 8/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 1.9201 - acc: 0.2715 - val_loss: 1.7929 - val_acc: 0.3148\n",
      "Epoch 9/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 1.8562 - acc: 0.2899 - val_loss: 1.7372 - val_acc: 0.3778\n",
      "Epoch 10/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 1.7860 - acc: 0.3059 - val_loss: 1.7204 - val_acc: 0.3001\n",
      "Epoch 11/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 1.7668 - acc: 0.3300 - val_loss: 1.6682 - val_acc: 0.4005\n",
      "Epoch 12/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 1.7260 - acc: 0.3472 - val_loss: 1.6414 - val_acc: 0.4023\n",
      "Epoch 13/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 1.7063 - acc: 0.3430 - val_loss: 1.6324 - val_acc: 0.4121\n",
      "Epoch 14/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 1.6951 - acc: 0.3508 - val_loss: 1.5922 - val_acc: 0.4427\n",
      "Epoch 15/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 1.6698 - acc: 0.3611 - val_loss: 1.5908 - val_acc: 0.4207\n",
      "Epoch 16/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 1.6452 - acc: 0.3816 - val_loss: 1.5754 - val_acc: 0.4311\n",
      "Epoch 17/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 1.5977 - acc: 0.4075 - val_loss: 1.5343 - val_acc: 0.4691\n",
      "Epoch 18/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 1.5899 - acc: 0.4030 - val_loss: 1.5315 - val_acc: 0.4672\n",
      "Epoch 19/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 1.5693 - acc: 0.4090 - val_loss: 1.5101 - val_acc: 0.4868\n",
      "Epoch 20/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 1.5523 - acc: 0.4115 - val_loss: 1.5064 - val_acc: 0.4452\n",
      "Epoch 21/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 1.5304 - acc: 0.4262 - val_loss: 1.4655 - val_acc: 0.4917\n",
      "Epoch 22/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 1.5235 - acc: 0.4335 - val_loss: 1.4705 - val_acc: 0.4770\n",
      "Epoch 23/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 1.4948 - acc: 0.4407 - val_loss: 1.4749 - val_acc: 0.4574\n",
      "Epoch 24/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 1.4862 - acc: 0.4504 - val_loss: 1.4596 - val_acc: 0.4832\n",
      "Epoch 25/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 1.4632 - acc: 0.4489 - val_loss: 1.4291 - val_acc: 0.4954\n",
      "Epoch 26/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 1.4607 - acc: 0.4416 - val_loss: 1.4098 - val_acc: 0.5089\n",
      "Epoch 27/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 1.4519 - acc: 0.4543 - val_loss: 1.4124 - val_acc: 0.5028\n",
      "Epoch 28/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 1.4402 - acc: 0.4688 - val_loss: 1.4431 - val_acc: 0.4568\n",
      "Epoch 29/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 1.4327 - acc: 0.4679 - val_loss: 1.3959 - val_acc: 0.5003\n",
      "Epoch 30/1000\n",
      "3315/3315 [==============================] - 2s 508us/step - loss: 1.4100 - acc: 0.4793 - val_loss: 1.3746 - val_acc: 0.5230\n",
      "Epoch 31/1000\n",
      "3315/3315 [==============================] - 2s 510us/step - loss: 1.4093 - acc: 0.4712 - val_loss: 1.3551 - val_acc: 0.5193\n",
      "Epoch 32/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 1.3878 - acc: 0.4917 - val_loss: 1.3410 - val_acc: 0.5291\n",
      "Epoch 33/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 1.3773 - acc: 0.5011 - val_loss: 1.3482 - val_acc: 0.5309\n",
      "Epoch 34/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 1.3657 - acc: 0.4950 - val_loss: 1.3290 - val_acc: 0.5456\n",
      "Epoch 35/1000\n",
      "3315/3315 [==============================] - 2s 508us/step - loss: 1.3722 - acc: 0.5035 - val_loss: 1.3277 - val_acc: 0.5168\n",
      "Epoch 36/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 1.3600 - acc: 0.4965 - val_loss: 1.3240 - val_acc: 0.5095\n",
      "Epoch 37/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 1.3511 - acc: 0.5032 - val_loss: 1.3159 - val_acc: 0.5321\n",
      "Epoch 38/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 1.3234 - acc: 0.5062 - val_loss: 1.2910 - val_acc: 0.5419\n",
      "Epoch 39/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 1.3202 - acc: 0.5125 - val_loss: 1.2772 - val_acc: 0.5481\n",
      "Epoch 40/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 1.3108 - acc: 0.5155 - val_loss: 1.2903 - val_acc: 0.5468\n",
      "Epoch 41/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 1.3073 - acc: 0.5210 - val_loss: 1.2677 - val_acc: 0.5542\n",
      "Epoch 42/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 1.3073 - acc: 0.5173 - val_loss: 1.2614 - val_acc: 0.5542\n",
      "Epoch 43/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 1.2786 - acc: 0.5276 - val_loss: 1.2519 - val_acc: 0.5499\n",
      "Epoch 44/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 1.2770 - acc: 0.5321 - val_loss: 1.2607 - val_acc: 0.5456\n",
      "Epoch 45/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 1.2725 - acc: 0.5294 - val_loss: 1.2543 - val_acc: 0.5585\n",
      "Epoch 46/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 1.2671 - acc: 0.5391 - val_loss: 1.2411 - val_acc: 0.5634\n",
      "Epoch 47/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 1.2527 - acc: 0.5454 - val_loss: 1.2308 - val_acc: 0.5530\n",
      "Epoch 48/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 1.2595 - acc: 0.5351 - val_loss: 1.2096 - val_acc: 0.5928\n",
      "Epoch 49/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 1.2308 - acc: 0.5490 - val_loss: 1.2453 - val_acc: 0.5628\n",
      "Epoch 50/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 1.2276 - acc: 0.5505 - val_loss: 1.2110 - val_acc: 0.5732\n",
      "Epoch 51/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 1.2154 - acc: 0.5478 - val_loss: 1.2097 - val_acc: 0.5701\n",
      "Epoch 52/1000\n",
      "3315/3315 [==============================] - 2s 478us/step - loss: 1.2143 - acc: 0.5575 - val_loss: 1.1858 - val_acc: 0.5952\n",
      "Epoch 53/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 1.1962 - acc: 0.5581 - val_loss: 1.2067 - val_acc: 0.5652\n",
      "Epoch 54/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 1.1932 - acc: 0.5605 - val_loss: 1.1683 - val_acc: 0.5854\n",
      "Epoch 55/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 1.1873 - acc: 0.5689 - val_loss: 1.1670 - val_acc: 0.5915\n",
      "Epoch 56/1000\n",
      "3315/3315 [==============================] - 2s 479us/step - loss: 1.1757 - acc: 0.5650 - val_loss: 1.1543 - val_acc: 0.5952\n",
      "Epoch 57/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 1.1700 - acc: 0.5710 - val_loss: 1.1538 - val_acc: 0.5915\n",
      "Epoch 58/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 1.1574 - acc: 0.5771 - val_loss: 1.1475 - val_acc: 0.5958\n",
      "Epoch 59/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 1.1704 - acc: 0.5729 - val_loss: 1.1377 - val_acc: 0.6044\n",
      "Epoch 60/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 1.1658 - acc: 0.5756 - val_loss: 1.1621 - val_acc: 0.5989\n",
      "Epoch 61/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 1.1546 - acc: 0.5741 - val_loss: 1.1271 - val_acc: 0.5989\n",
      "Epoch 62/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 1.1420 - acc: 0.5846 - val_loss: 1.1353 - val_acc: 0.5909\n",
      "Epoch 63/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 1.1459 - acc: 0.5798 - val_loss: 1.1345 - val_acc: 0.5928\n",
      "Epoch 64/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 1.1353 - acc: 0.5934 - val_loss: 1.1235 - val_acc: 0.6081\n",
      "Epoch 65/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 1.1209 - acc: 0.5876 - val_loss: 1.1221 - val_acc: 0.5897\n",
      "Epoch 66/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 1.1126 - acc: 0.5831 - val_loss: 1.1076 - val_acc: 0.6075\n",
      "Epoch 67/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 1.1107 - acc: 0.5922 - val_loss: 1.0963 - val_acc: 0.6173\n",
      "Epoch 68/1000\n",
      "3315/3315 [==============================] - 2s 509us/step - loss: 1.1084 - acc: 0.5994 - val_loss: 1.1084 - val_acc: 0.6020\n",
      "Epoch 69/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 1.1134 - acc: 0.5922 - val_loss: 1.0914 - val_acc: 0.6142\n",
      "Epoch 70/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 1.1026 - acc: 0.5970 - val_loss: 1.1052 - val_acc: 0.6081\n",
      "Epoch 71/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 1.0922 - acc: 0.6045 - val_loss: 1.0879 - val_acc: 0.6099\n",
      "Epoch 72/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 1.0841 - acc: 0.6018 - val_loss: 1.0749 - val_acc: 0.6295\n",
      "Epoch 73/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 1.0744 - acc: 0.6066 - val_loss: 1.0878 - val_acc: 0.6222\n",
      "Epoch 74/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 1.0839 - acc: 0.6045 - val_loss: 1.0591 - val_acc: 0.6289\n",
      "Epoch 75/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 1.0775 - acc: 0.6106 - val_loss: 1.0620 - val_acc: 0.6173\n",
      "Epoch 76/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 1.0696 - acc: 0.6048 - val_loss: 1.0670 - val_acc: 0.6203\n",
      "Epoch 77/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 1.0595 - acc: 0.6211 - val_loss: 1.0522 - val_acc: 0.6265\n",
      "Epoch 78/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 1.0440 - acc: 0.6232 - val_loss: 1.0811 - val_acc: 0.6087\n",
      "Epoch 79/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 1.0455 - acc: 0.6163 - val_loss: 1.0296 - val_acc: 0.6454\n",
      "Epoch 80/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 1.0435 - acc: 0.6262 - val_loss: 1.0460 - val_acc: 0.6197\n",
      "Epoch 81/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 1.0395 - acc: 0.6281 - val_loss: 1.0352 - val_acc: 0.6277\n",
      "Epoch 82/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 1.0433 - acc: 0.6115 - val_loss: 1.0358 - val_acc: 0.6295\n",
      "Epoch 83/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 1.0325 - acc: 0.6253 - val_loss: 1.0429 - val_acc: 0.6295\n",
      "Epoch 84/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 1.0247 - acc: 0.6284 - val_loss: 1.0348 - val_acc: 0.6320\n",
      "Epoch 85/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 1.0146 - acc: 0.6338 - val_loss: 1.0238 - val_acc: 0.6497\n",
      "Epoch 86/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 1.0272 - acc: 0.6205 - val_loss: 1.0083 - val_acc: 0.6369\n",
      "Epoch 87/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 1.0193 - acc: 0.6320 - val_loss: 1.0346 - val_acc: 0.6283\n",
      "Epoch 88/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 1.0141 - acc: 0.6268 - val_loss: 1.0196 - val_acc: 0.6399\n",
      "Epoch 89/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.9977 - acc: 0.6425 - val_loss: 1.0067 - val_acc: 0.6509\n",
      "Epoch 90/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.9913 - acc: 0.6341 - val_loss: 0.9940 - val_acc: 0.6546\n",
      "Epoch 91/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 1.0007 - acc: 0.6398 - val_loss: 0.9968 - val_acc: 0.6454\n",
      "Epoch 92/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.9766 - acc: 0.6498 - val_loss: 0.9841 - val_acc: 0.6601\n",
      "Epoch 93/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.9931 - acc: 0.6401 - val_loss: 0.9830 - val_acc: 0.6589\n",
      "Epoch 94/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.9777 - acc: 0.6446 - val_loss: 0.9871 - val_acc: 0.6418\n",
      "Epoch 95/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.9762 - acc: 0.6425 - val_loss: 0.9927 - val_acc: 0.6326\n",
      "Epoch 96/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.9619 - acc: 0.6489 - val_loss: 0.9732 - val_acc: 0.6430\n",
      "Epoch 97/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.9685 - acc: 0.6480 - val_loss: 0.9830 - val_acc: 0.6454\n",
      "Epoch 98/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.9661 - acc: 0.6437 - val_loss: 0.9827 - val_acc: 0.6516\n",
      "Epoch 99/1000\n",
      "3315/3315 [==============================] - 2s 518us/step - loss: 0.9676 - acc: 0.6486 - val_loss: 0.9776 - val_acc: 0.6589\n",
      "Epoch 100/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.9716 - acc: 0.6419 - val_loss: 0.9665 - val_acc: 0.6565\n",
      "Epoch 101/1000\n",
      "3315/3315 [==============================] - 2s 510us/step - loss: 0.9516 - acc: 0.6528 - val_loss: 0.9623 - val_acc: 0.6589\n",
      "Epoch 102/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.9538 - acc: 0.6597 - val_loss: 0.9557 - val_acc: 0.6687\n",
      "Epoch 103/1000\n",
      "3315/3315 [==============================] - 2s 509us/step - loss: 0.9452 - acc: 0.6600 - val_loss: 0.9616 - val_acc: 0.6638\n",
      "Epoch 104/1000\n",
      "3315/3315 [==============================] - 2s 512us/step - loss: 0.9404 - acc: 0.6591 - val_loss: 0.9631 - val_acc: 0.6436\n",
      "Epoch 105/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.9353 - acc: 0.6549 - val_loss: 0.9636 - val_acc: 0.6497\n",
      "Epoch 106/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.9393 - acc: 0.6588 - val_loss: 0.9561 - val_acc: 0.6485\n",
      "Epoch 107/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.9271 - acc: 0.6661 - val_loss: 0.9533 - val_acc: 0.6442\n",
      "Epoch 108/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.9237 - acc: 0.6694 - val_loss: 0.9472 - val_acc: 0.6712\n",
      "Epoch 109/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.9300 - acc: 0.6603 - val_loss: 0.9427 - val_acc: 0.6540\n",
      "Epoch 110/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.9212 - acc: 0.6661 - val_loss: 0.9658 - val_acc: 0.6350\n",
      "Epoch 111/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.9192 - acc: 0.6570 - val_loss: 0.9339 - val_acc: 0.6614\n",
      "Epoch 112/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.9169 - acc: 0.6799 - val_loss: 0.9271 - val_acc: 0.6589\n",
      "Epoch 113/1000\n",
      "3315/3315 [==============================] - 2s 528us/step - loss: 0.8997 - acc: 0.6715 - val_loss: 0.9233 - val_acc: 0.6681\n",
      "Epoch 114/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.9017 - acc: 0.6709 - val_loss: 0.9109 - val_acc: 0.6761\n",
      "Epoch 115/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.8977 - acc: 0.6739 - val_loss: 0.9602 - val_acc: 0.6418\n",
      "Epoch 116/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.9037 - acc: 0.6775 - val_loss: 0.9274 - val_acc: 0.6693\n",
      "Epoch 117/1000\n",
      "3315/3315 [==============================] - 2s 543us/step - loss: 0.8973 - acc: 0.6742 - val_loss: 0.9105 - val_acc: 0.6687\n",
      "Epoch 118/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.8739 - acc: 0.6839 - val_loss: 0.9454 - val_acc: 0.6540\n",
      "Epoch 119/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.8850 - acc: 0.6869 - val_loss: 0.9393 - val_acc: 0.6528\n",
      "Epoch 120/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.8756 - acc: 0.6805 - val_loss: 0.9138 - val_acc: 0.6699\n",
      "Epoch 121/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.8838 - acc: 0.6836 - val_loss: 0.9098 - val_acc: 0.6583\n",
      "Epoch 122/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.8656 - acc: 0.6887 - val_loss: 0.8977 - val_acc: 0.6797\n",
      "Epoch 123/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.8613 - acc: 0.7026 - val_loss: 0.9354 - val_acc: 0.6644\n",
      "Epoch 124/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.8700 - acc: 0.6802 - val_loss: 0.8987 - val_acc: 0.6669\n",
      "Epoch 125/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.8497 - acc: 0.6959 - val_loss: 0.8858 - val_acc: 0.6846\n",
      "Epoch 126/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.8615 - acc: 0.6932 - val_loss: 0.9022 - val_acc: 0.6663\n",
      "Epoch 127/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.8569 - acc: 0.6941 - val_loss: 0.8732 - val_acc: 0.6926\n",
      "Epoch 128/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.8472 - acc: 0.6959 - val_loss: 0.8881 - val_acc: 0.6742\n",
      "Epoch 129/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.8478 - acc: 0.6923 - val_loss: 0.8852 - val_acc: 0.6754\n",
      "Epoch 130/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.8465 - acc: 0.6917 - val_loss: 0.8906 - val_acc: 0.6650\n",
      "Epoch 131/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.8428 - acc: 0.6956 - val_loss: 0.8763 - val_acc: 0.6803\n",
      "Epoch 132/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.8404 - acc: 0.6953 - val_loss: 0.8888 - val_acc: 0.6828\n",
      "Epoch 133/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.8311 - acc: 0.7011 - val_loss: 0.8773 - val_acc: 0.6840\n",
      "Epoch 134/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.8358 - acc: 0.7065 - val_loss: 0.8896 - val_acc: 0.6767\n",
      "Epoch 135/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.8178 - acc: 0.7032 - val_loss: 0.8638 - val_acc: 0.6877\n",
      "Epoch 136/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.8234 - acc: 0.7020 - val_loss: 0.8609 - val_acc: 0.6895\n",
      "Epoch 137/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.8164 - acc: 0.7098 - val_loss: 0.9042 - val_acc: 0.6577\n",
      "Epoch 138/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 0.8187 - acc: 0.7002 - val_loss: 0.8652 - val_acc: 0.6889\n",
      "Epoch 139/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.8231 - acc: 0.7029 - val_loss: 0.8515 - val_acc: 0.6895\n",
      "Epoch 140/1000\n",
      "3315/3315 [==============================] - 2s 509us/step - loss: 0.8160 - acc: 0.7101 - val_loss: 0.8487 - val_acc: 0.7018\n",
      "Epoch 141/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.8010 - acc: 0.7140 - val_loss: 0.8641 - val_acc: 0.6822\n",
      "Epoch 142/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.8054 - acc: 0.7176 - val_loss: 0.8682 - val_acc: 0.6920\n",
      "Epoch 143/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.7938 - acc: 0.7164 - val_loss: 0.8409 - val_acc: 0.7036\n",
      "Epoch 144/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.7980 - acc: 0.7086 - val_loss: 0.8730 - val_acc: 0.6718\n",
      "Epoch 145/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.7862 - acc: 0.7095 - val_loss: 0.8625 - val_acc: 0.6852\n",
      "Epoch 146/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.7954 - acc: 0.7137 - val_loss: 0.8873 - val_acc: 0.6681\n",
      "Epoch 147/1000\n",
      "3315/3315 [==============================] - 2s 478us/step - loss: 0.7950 - acc: 0.7059 - val_loss: 0.8294 - val_acc: 0.6999\n",
      "Epoch 148/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.7930 - acc: 0.7149 - val_loss: 0.8144 - val_acc: 0.7177\n",
      "Epoch 149/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.7731 - acc: 0.7264 - val_loss: 0.8288 - val_acc: 0.6950\n",
      "Epoch 150/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.7850 - acc: 0.7125 - val_loss: 0.8387 - val_acc: 0.6963\n",
      "Epoch 151/1000\n",
      "3315/3315 [==============================] - 2s 547us/step - loss: 0.7755 - acc: 0.7240 - val_loss: 0.8505 - val_acc: 0.6963\n",
      "Epoch 152/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.7629 - acc: 0.7240 - val_loss: 0.8417 - val_acc: 0.6938\n",
      "Epoch 153/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.7669 - acc: 0.7261 - val_loss: 0.8240 - val_acc: 0.6975\n",
      "Epoch 154/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.7712 - acc: 0.7225 - val_loss: 0.8089 - val_acc: 0.7171\n",
      "Epoch 155/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.7673 - acc: 0.7234 - val_loss: 0.8416 - val_acc: 0.6834\n",
      "Epoch 156/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.7644 - acc: 0.7219 - val_loss: 0.8250 - val_acc: 0.7036\n",
      "Epoch 157/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.7608 - acc: 0.7222 - val_loss: 0.8046 - val_acc: 0.7140\n",
      "Epoch 158/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.7548 - acc: 0.7312 - val_loss: 0.8009 - val_acc: 0.7097\n",
      "Epoch 159/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.7629 - acc: 0.7300 - val_loss: 0.8115 - val_acc: 0.7067\n",
      "Epoch 160/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.7556 - acc: 0.7279 - val_loss: 0.8257 - val_acc: 0.6987\n",
      "Epoch 161/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.7612 - acc: 0.7246 - val_loss: 0.7835 - val_acc: 0.7238\n",
      "Epoch 162/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.7490 - acc: 0.7312 - val_loss: 0.8031 - val_acc: 0.7073\n",
      "Epoch 163/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.7451 - acc: 0.7330 - val_loss: 0.7971 - val_acc: 0.7165\n",
      "Epoch 164/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.7426 - acc: 0.7246 - val_loss: 0.8055 - val_acc: 0.7030\n",
      "Epoch 165/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.7380 - acc: 0.7348 - val_loss: 0.8096 - val_acc: 0.7036\n",
      "Epoch 166/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.7432 - acc: 0.7300 - val_loss: 0.7901 - val_acc: 0.7208\n",
      "Epoch 167/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.7412 - acc: 0.7297 - val_loss: 0.7719 - val_acc: 0.7318\n",
      "Epoch 168/1000\n",
      "3315/3315 [==============================] - 2s 473us/step - loss: 0.7365 - acc: 0.7363 - val_loss: 0.8102 - val_acc: 0.7122\n",
      "Epoch 169/1000\n",
      "3315/3315 [==============================] - 2s 477us/step - loss: 0.7330 - acc: 0.7357 - val_loss: 0.7848 - val_acc: 0.7165\n",
      "Epoch 170/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.7184 - acc: 0.7478 - val_loss: 0.7809 - val_acc: 0.7122\n",
      "Epoch 171/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.7276 - acc: 0.7382 - val_loss: 0.7927 - val_acc: 0.7128\n",
      "Epoch 172/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.7281 - acc: 0.7460 - val_loss: 0.7639 - val_acc: 0.7318\n",
      "Epoch 173/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.7141 - acc: 0.7451 - val_loss: 0.7745 - val_acc: 0.7116\n",
      "Epoch 174/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.7145 - acc: 0.7403 - val_loss: 0.7773 - val_acc: 0.7263\n",
      "Epoch 175/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.7099 - acc: 0.7499 - val_loss: 0.7772 - val_acc: 0.7220\n",
      "Epoch 176/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.7105 - acc: 0.7433 - val_loss: 0.7935 - val_acc: 0.7165\n",
      "Epoch 177/1000\n",
      "3315/3315 [==============================] - 2s 512us/step - loss: 0.7007 - acc: 0.7544 - val_loss: 0.7636 - val_acc: 0.7263\n",
      "Epoch 178/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.7066 - acc: 0.7397 - val_loss: 0.7609 - val_acc: 0.7373\n",
      "Epoch 179/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.7183 - acc: 0.7397 - val_loss: 0.8017 - val_acc: 0.7030\n",
      "Epoch 180/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.6935 - acc: 0.7457 - val_loss: 0.7499 - val_acc: 0.7257\n",
      "Epoch 181/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.7025 - acc: 0.7442 - val_loss: 0.7618 - val_acc: 0.7336\n",
      "Epoch 182/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.6996 - acc: 0.7409 - val_loss: 0.7473 - val_acc: 0.7428\n",
      "Epoch 183/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.7029 - acc: 0.7511 - val_loss: 0.7498 - val_acc: 0.7299\n",
      "Epoch 184/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.6780 - acc: 0.7611 - val_loss: 0.7444 - val_acc: 0.7342\n",
      "Epoch 185/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.6976 - acc: 0.7445 - val_loss: 0.7675 - val_acc: 0.7355\n",
      "Epoch 186/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.6889 - acc: 0.7538 - val_loss: 0.7377 - val_acc: 0.7404\n",
      "Epoch 187/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.6950 - acc: 0.7475 - val_loss: 0.7580 - val_acc: 0.7244\n",
      "Epoch 188/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.6880 - acc: 0.7548 - val_loss: 0.7551 - val_acc: 0.7214\n",
      "Epoch 189/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.6840 - acc: 0.7541 - val_loss: 0.7470 - val_acc: 0.7373\n",
      "Epoch 190/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.6832 - acc: 0.7514 - val_loss: 0.7344 - val_acc: 0.7465\n",
      "Epoch 191/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.6780 - acc: 0.7599 - val_loss: 0.7225 - val_acc: 0.7428\n",
      "Epoch 192/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.6719 - acc: 0.7566 - val_loss: 0.7432 - val_acc: 0.7232\n",
      "Epoch 193/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.6791 - acc: 0.7532 - val_loss: 0.7386 - val_acc: 0.7330\n",
      "Epoch 194/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.6789 - acc: 0.7560 - val_loss: 0.7319 - val_acc: 0.7489\n",
      "Epoch 195/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.6732 - acc: 0.7602 - val_loss: 0.7399 - val_acc: 0.7281\n",
      "Epoch 196/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.6626 - acc: 0.7602 - val_loss: 0.7340 - val_acc: 0.7355\n",
      "Epoch 197/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.6653 - acc: 0.7638 - val_loss: 0.7314 - val_acc: 0.7434\n",
      "Epoch 198/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.6565 - acc: 0.7596 - val_loss: 0.7528 - val_acc: 0.7306\n",
      "Epoch 199/1000\n",
      "3315/3315 [==============================] - 2s 513us/step - loss: 0.6578 - acc: 0.7538 - val_loss: 0.7296 - val_acc: 0.7355\n",
      "Epoch 200/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.6553 - acc: 0.7653 - val_loss: 0.7260 - val_acc: 0.7404\n",
      "Epoch 201/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.6546 - acc: 0.7629 - val_loss: 0.7657 - val_acc: 0.7287\n",
      "Epoch 202/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.6569 - acc: 0.7677 - val_loss: 0.7141 - val_acc: 0.7538\n",
      "Epoch 203/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.6491 - acc: 0.7683 - val_loss: 0.7094 - val_acc: 0.7502\n",
      "Epoch 204/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.6435 - acc: 0.7668 - val_loss: 0.7259 - val_acc: 0.7355\n",
      "Epoch 205/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.6328 - acc: 0.7735 - val_loss: 0.7364 - val_acc: 0.7336\n",
      "Epoch 206/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.6352 - acc: 0.7680 - val_loss: 0.7543 - val_acc: 0.7250\n",
      "Epoch 207/1000\n",
      "3315/3315 [==============================] - 2s 509us/step - loss: 0.6346 - acc: 0.7753 - val_loss: 0.7152 - val_acc: 0.7397\n",
      "Epoch 208/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.6219 - acc: 0.7801 - val_loss: 0.7072 - val_acc: 0.7477\n",
      "Epoch 209/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.6366 - acc: 0.7695 - val_loss: 0.7174 - val_acc: 0.7391\n",
      "Epoch 210/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 0.6361 - acc: 0.7789 - val_loss: 0.6889 - val_acc: 0.7685\n",
      "Epoch 211/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.6332 - acc: 0.7719 - val_loss: 0.7164 - val_acc: 0.7459\n",
      "Epoch 212/1000\n",
      "3315/3315 [==============================] - 2s 514us/step - loss: 0.6242 - acc: 0.7816 - val_loss: 0.6970 - val_acc: 0.7551\n",
      "Epoch 213/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.6236 - acc: 0.7765 - val_loss: 0.6966 - val_acc: 0.7514\n",
      "Epoch 214/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.6289 - acc: 0.7704 - val_loss: 0.6954 - val_acc: 0.7612\n",
      "Epoch 215/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.6292 - acc: 0.7738 - val_loss: 0.6999 - val_acc: 0.7489\n",
      "Epoch 216/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.6115 - acc: 0.7834 - val_loss: 0.7299 - val_acc: 0.7232\n",
      "Epoch 217/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.6092 - acc: 0.7789 - val_loss: 0.6926 - val_acc: 0.7532\n",
      "Epoch 218/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.6137 - acc: 0.7819 - val_loss: 0.6988 - val_acc: 0.7532\n",
      "Epoch 219/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.6081 - acc: 0.7828 - val_loss: 0.7259 - val_acc: 0.7373\n",
      "Epoch 220/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.6065 - acc: 0.7813 - val_loss: 0.6991 - val_acc: 0.7569\n",
      "Epoch 221/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.6186 - acc: 0.7771 - val_loss: 0.6710 - val_acc: 0.7685\n",
      "Epoch 222/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.6053 - acc: 0.7843 - val_loss: 0.7276 - val_acc: 0.7299\n",
      "Epoch 223/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.6157 - acc: 0.7783 - val_loss: 0.6829 - val_acc: 0.7612\n",
      "Epoch 224/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.6029 - acc: 0.7864 - val_loss: 0.6784 - val_acc: 0.7587\n",
      "Epoch 225/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.5953 - acc: 0.7894 - val_loss: 0.7063 - val_acc: 0.7526\n",
      "Epoch 226/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.6021 - acc: 0.7849 - val_loss: 0.7132 - val_acc: 0.7330\n",
      "Epoch 227/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.5948 - acc: 0.7900 - val_loss: 0.6824 - val_acc: 0.7600\n",
      "Epoch 228/1000\n",
      "3315/3315 [==============================] - 2s 479us/step - loss: 0.5950 - acc: 0.7894 - val_loss: 0.6897 - val_acc: 0.7538\n",
      "Epoch 229/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.5876 - acc: 0.7885 - val_loss: 0.6726 - val_acc: 0.7612\n",
      "Epoch 230/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.5965 - acc: 0.7861 - val_loss: 0.6895 - val_acc: 0.7600\n",
      "Epoch 231/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.5850 - acc: 0.7903 - val_loss: 0.6966 - val_acc: 0.7563\n",
      "Epoch 232/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.5927 - acc: 0.7897 - val_loss: 0.6643 - val_acc: 0.7753\n",
      "Epoch 233/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.5853 - acc: 0.7885 - val_loss: 0.6635 - val_acc: 0.7734\n",
      "Epoch 234/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.5873 - acc: 0.7885 - val_loss: 0.6853 - val_acc: 0.7532\n",
      "Epoch 235/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.5744 - acc: 0.7934 - val_loss: 0.6803 - val_acc: 0.7624\n",
      "Epoch 236/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.5703 - acc: 0.7946 - val_loss: 0.7222 - val_acc: 0.7404\n",
      "Epoch 237/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.5867 - acc: 0.7897 - val_loss: 0.6774 - val_acc: 0.7661\n",
      "Epoch 238/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.5669 - acc: 0.7952 - val_loss: 0.6648 - val_acc: 0.7691\n",
      "Epoch 239/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.5833 - acc: 0.7919 - val_loss: 0.6559 - val_acc: 0.7746\n",
      "Epoch 240/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.5694 - acc: 0.7910 - val_loss: 0.6465 - val_acc: 0.7765\n",
      "Epoch 241/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.5757 - acc: 0.7922 - val_loss: 0.6633 - val_acc: 0.7685\n",
      "Epoch 242/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.5670 - acc: 0.8045 - val_loss: 0.6933 - val_acc: 0.7477\n",
      "Epoch 243/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.5617 - acc: 0.8048 - val_loss: 0.6827 - val_acc: 0.7465\n",
      "Epoch 244/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.5620 - acc: 0.8051 - val_loss: 0.6528 - val_acc: 0.7759\n",
      "Epoch 245/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.5683 - acc: 0.7991 - val_loss: 0.6570 - val_acc: 0.7789\n",
      "Epoch 246/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.5708 - acc: 0.7943 - val_loss: 0.6611 - val_acc: 0.7753\n",
      "Epoch 247/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.5666 - acc: 0.7970 - val_loss: 0.6500 - val_acc: 0.7753\n",
      "Epoch 248/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.5709 - acc: 0.8003 - val_loss: 0.6475 - val_acc: 0.7777\n",
      "Epoch 249/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.5640 - acc: 0.8009 - val_loss: 0.6337 - val_acc: 0.7863\n",
      "Epoch 250/1000\n",
      "3315/3315 [==============================] - 2s 513us/step - loss: 0.5597 - acc: 0.7973 - val_loss: 0.6634 - val_acc: 0.7606\n",
      "Epoch 251/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.5503 - acc: 0.7979 - val_loss: 0.6920 - val_acc: 0.7514\n",
      "Epoch 252/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.5620 - acc: 0.7967 - val_loss: 0.6461 - val_acc: 0.7783\n",
      "Epoch 253/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.5523 - acc: 0.7979 - val_loss: 0.6322 - val_acc: 0.7820\n",
      "Epoch 254/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.5404 - acc: 0.8078 - val_loss: 0.6332 - val_acc: 0.7808\n",
      "Epoch 255/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.5444 - acc: 0.8006 - val_loss: 0.6502 - val_acc: 0.7783\n",
      "Epoch 256/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.5483 - acc: 0.8042 - val_loss: 0.6360 - val_acc: 0.7783\n",
      "Epoch 257/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.5405 - acc: 0.8063 - val_loss: 0.6318 - val_acc: 0.7746\n",
      "Epoch 258/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.5332 - acc: 0.8097 - val_loss: 0.6387 - val_acc: 0.7844\n",
      "Epoch 259/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.5342 - acc: 0.8097 - val_loss: 0.6266 - val_acc: 0.7802\n",
      "Epoch 260/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.5494 - acc: 0.8009 - val_loss: 0.6527 - val_acc: 0.7685\n",
      "Epoch 261/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.5396 - acc: 0.8054 - val_loss: 0.6629 - val_acc: 0.7673\n",
      "Epoch 262/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.5340 - acc: 0.8066 - val_loss: 0.6320 - val_acc: 0.7746\n",
      "Epoch 263/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.5391 - acc: 0.8075 - val_loss: 0.6320 - val_acc: 0.7783\n",
      "Epoch 264/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.5315 - acc: 0.8106 - val_loss: 0.6063 - val_acc: 0.7875\n",
      "Epoch 265/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.5308 - acc: 0.8163 - val_loss: 0.6190 - val_acc: 0.7814\n",
      "Epoch 266/1000\n",
      "3315/3315 [==============================] - 2s 479us/step - loss: 0.5372 - acc: 0.8112 - val_loss: 0.6336 - val_acc: 0.7820\n",
      "Epoch 267/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.5267 - acc: 0.8139 - val_loss: 0.6147 - val_acc: 0.7881\n",
      "Epoch 268/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.5330 - acc: 0.8097 - val_loss: 0.6075 - val_acc: 0.7924\n",
      "Epoch 269/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.5215 - acc: 0.8190 - val_loss: 0.6146 - val_acc: 0.7900\n",
      "Epoch 270/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.5246 - acc: 0.8184 - val_loss: 0.6435 - val_acc: 0.7771\n",
      "Epoch 271/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.5208 - acc: 0.8154 - val_loss: 0.6222 - val_acc: 0.7783\n",
      "Epoch 272/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.5198 - acc: 0.8109 - val_loss: 0.6355 - val_acc: 0.7906\n",
      "Epoch 273/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.5161 - acc: 0.8157 - val_loss: 0.6372 - val_acc: 0.7697\n",
      "Epoch 274/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.5286 - acc: 0.8181 - val_loss: 0.6341 - val_acc: 0.7734\n",
      "Epoch 275/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.4974 - acc: 0.8214 - val_loss: 0.6084 - val_acc: 0.7857\n",
      "Epoch 276/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.5085 - acc: 0.8244 - val_loss: 0.5990 - val_acc: 0.7955\n",
      "Epoch 277/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.5224 - acc: 0.8060 - val_loss: 0.6113 - val_acc: 0.7838\n",
      "Epoch 278/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.5172 - acc: 0.8097 - val_loss: 0.5989 - val_acc: 0.8004\n",
      "Epoch 279/1000\n",
      "3315/3315 [==============================] - 2s 477us/step - loss: 0.4995 - acc: 0.8265 - val_loss: 0.6124 - val_acc: 0.7955\n",
      "Epoch 280/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.5089 - acc: 0.8184 - val_loss: 0.6108 - val_acc: 0.7900\n",
      "Epoch 281/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.5010 - acc: 0.8247 - val_loss: 0.6215 - val_acc: 0.7844\n",
      "Epoch 282/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.5102 - acc: 0.8223 - val_loss: 0.6091 - val_acc: 0.7881\n",
      "Epoch 283/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.5056 - acc: 0.8214 - val_loss: 0.6238 - val_acc: 0.7746\n",
      "Epoch 284/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.4894 - acc: 0.8253 - val_loss: 0.5926 - val_acc: 0.8010\n",
      "Epoch 285/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.5001 - acc: 0.8284 - val_loss: 0.6183 - val_acc: 0.7863\n",
      "Epoch 286/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.5031 - acc: 0.8211 - val_loss: 0.6058 - val_acc: 0.7851\n",
      "Epoch 287/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.4930 - acc: 0.8299 - val_loss: 0.6090 - val_acc: 0.7740\n",
      "Epoch 288/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.4965 - acc: 0.8299 - val_loss: 0.6065 - val_acc: 0.8016\n",
      "Epoch 289/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.4812 - acc: 0.8347 - val_loss: 0.6140 - val_acc: 0.7912\n",
      "Epoch 290/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.4809 - acc: 0.8284 - val_loss: 0.5834 - val_acc: 0.8010\n",
      "Epoch 291/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.5051 - acc: 0.8259 - val_loss: 0.5983 - val_acc: 0.7967\n",
      "Epoch 292/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.4860 - acc: 0.8232 - val_loss: 0.5790 - val_acc: 0.8059\n",
      "Epoch 293/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.4857 - acc: 0.8287 - val_loss: 0.6035 - val_acc: 0.7869\n",
      "Epoch 294/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.4821 - acc: 0.8281 - val_loss: 0.6053 - val_acc: 0.7887\n",
      "Epoch 295/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.4755 - acc: 0.8281 - val_loss: 0.5860 - val_acc: 0.8059\n",
      "Epoch 296/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.4865 - acc: 0.8338 - val_loss: 0.5685 - val_acc: 0.8120\n",
      "Epoch 297/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.4830 - acc: 0.8356 - val_loss: 0.6014 - val_acc: 0.7936\n",
      "Epoch 298/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.4826 - acc: 0.8347 - val_loss: 0.5961 - val_acc: 0.7973\n",
      "Epoch 299/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.4720 - acc: 0.8392 - val_loss: 0.5837 - val_acc: 0.8132\n",
      "Epoch 300/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.4686 - acc: 0.8305 - val_loss: 0.5760 - val_acc: 0.8010\n",
      "Epoch 301/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.4586 - acc: 0.8404 - val_loss: 0.5914 - val_acc: 0.7930\n",
      "Epoch 302/1000\n",
      "3315/3315 [==============================] - 2s 520us/step - loss: 0.4680 - acc: 0.8428 - val_loss: 0.5776 - val_acc: 0.7985\n",
      "Epoch 303/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.4696 - acc: 0.8368 - val_loss: 0.5873 - val_acc: 0.7979\n",
      "Epoch 304/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.4589 - acc: 0.8392 - val_loss: 0.5762 - val_acc: 0.8004\n",
      "Epoch 305/1000\n",
      "3315/3315 [==============================] - 2s 553us/step - loss: 0.4581 - acc: 0.8407 - val_loss: 0.5779 - val_acc: 0.8059\n",
      "Epoch 306/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.4771 - acc: 0.8278 - val_loss: 0.5835 - val_acc: 0.7955\n",
      "Epoch 307/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.4676 - acc: 0.8332 - val_loss: 0.5590 - val_acc: 0.8200\n",
      "Epoch 308/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.4653 - acc: 0.8386 - val_loss: 0.5791 - val_acc: 0.7936\n",
      "Epoch 309/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.4543 - acc: 0.8377 - val_loss: 0.5755 - val_acc: 0.8022\n",
      "Epoch 310/1000\n",
      "3315/3315 [==============================] - 2s 477us/step - loss: 0.4653 - acc: 0.8353 - val_loss: 0.5620 - val_acc: 0.8187\n",
      "Epoch 311/1000\n",
      "3315/3315 [==============================] - 2s 473us/step - loss: 0.4658 - acc: 0.8341 - val_loss: 0.5667 - val_acc: 0.8053\n",
      "Epoch 312/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.4618 - acc: 0.8419 - val_loss: 0.5946 - val_acc: 0.7900\n",
      "Epoch 313/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.4529 - acc: 0.8398 - val_loss: 0.5676 - val_acc: 0.8059\n",
      "Epoch 314/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.4703 - acc: 0.8305 - val_loss: 0.5630 - val_acc: 0.8040\n",
      "Epoch 315/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.4577 - acc: 0.8449 - val_loss: 0.5677 - val_acc: 0.8065\n",
      "Epoch 316/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.4584 - acc: 0.8341 - val_loss: 0.5509 - val_acc: 0.8249\n",
      "Epoch 317/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.4575 - acc: 0.8395 - val_loss: 0.5609 - val_acc: 0.8065\n",
      "Epoch 318/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.4426 - acc: 0.8480 - val_loss: 0.5504 - val_acc: 0.8169\n",
      "Epoch 319/1000\n",
      "3315/3315 [==============================] - 2s 509us/step - loss: 0.4475 - acc: 0.8398 - val_loss: 0.5739 - val_acc: 0.8053\n",
      "Epoch 320/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.4650 - acc: 0.8377 - val_loss: 0.5359 - val_acc: 0.8267\n",
      "Epoch 321/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.4561 - acc: 0.8419 - val_loss: 0.5445 - val_acc: 0.8175\n",
      "Epoch 322/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.4426 - acc: 0.8416 - val_loss: 0.5503 - val_acc: 0.8181\n",
      "Epoch 323/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 0.4572 - acc: 0.8407 - val_loss: 0.5473 - val_acc: 0.8120\n",
      "Epoch 324/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.4567 - acc: 0.8452 - val_loss: 0.5492 - val_acc: 0.8200\n",
      "Epoch 325/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.4532 - acc: 0.8401 - val_loss: 0.5480 - val_acc: 0.8200\n",
      "Epoch 326/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.4392 - acc: 0.8465 - val_loss: 0.5441 - val_acc: 0.8218\n",
      "Epoch 327/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.4299 - acc: 0.8480 - val_loss: 0.5459 - val_acc: 0.8181\n",
      "Epoch 328/1000\n",
      "3315/3315 [==============================] - 2s 510us/step - loss: 0.4452 - acc: 0.8416 - val_loss: 0.5512 - val_acc: 0.8151\n",
      "Epoch 329/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.4405 - acc: 0.8383 - val_loss: 0.5605 - val_acc: 0.8114\n",
      "Epoch 330/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.4373 - acc: 0.8486 - val_loss: 0.5375 - val_acc: 0.8206\n",
      "Epoch 331/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.4462 - acc: 0.8431 - val_loss: 0.5595 - val_acc: 0.8083\n",
      "Epoch 332/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.4205 - acc: 0.8516 - val_loss: 0.5581 - val_acc: 0.8145\n",
      "Epoch 333/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.4256 - acc: 0.8519 - val_loss: 0.5418 - val_acc: 0.8236\n",
      "Epoch 334/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.4240 - acc: 0.8534 - val_loss: 0.5397 - val_acc: 0.8175\n",
      "Epoch 335/1000\n",
      "3315/3315 [==============================] - 2s 552us/step - loss: 0.4256 - acc: 0.8549 - val_loss: 0.5502 - val_acc: 0.8077\n",
      "Epoch 336/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.4401 - acc: 0.8510 - val_loss: 0.5772 - val_acc: 0.7998\n",
      "Epoch 337/1000\n",
      "3315/3315 [==============================] - 2s 542us/step - loss: 0.4353 - acc: 0.8492 - val_loss: 0.5157 - val_acc: 0.8359\n",
      "Epoch 338/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.4137 - acc: 0.8465 - val_loss: 0.5697 - val_acc: 0.7991\n",
      "Epoch 339/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.4184 - acc: 0.8552 - val_loss: 0.5381 - val_acc: 0.8230\n",
      "Epoch 340/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.4165 - acc: 0.8549 - val_loss: 0.5594 - val_acc: 0.8126\n",
      "Epoch 341/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.4347 - acc: 0.8486 - val_loss: 0.5417 - val_acc: 0.8194\n",
      "Epoch 342/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.4287 - acc: 0.8498 - val_loss: 0.5504 - val_acc: 0.8132\n",
      "Epoch 343/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.4172 - acc: 0.8471 - val_loss: 0.5311 - val_acc: 0.8218\n",
      "Epoch 344/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.4326 - acc: 0.8504 - val_loss: 0.5466 - val_acc: 0.8145\n",
      "Epoch 345/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 0.4199 - acc: 0.8534 - val_loss: 0.5337 - val_acc: 0.8187\n",
      "Epoch 346/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.4274 - acc: 0.8510 - val_loss: 0.5217 - val_acc: 0.8255\n",
      "Epoch 347/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.4066 - acc: 0.8549 - val_loss: 0.5321 - val_acc: 0.8242\n",
      "Epoch 348/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.4006 - acc: 0.8585 - val_loss: 0.5368 - val_acc: 0.8157\n",
      "Epoch 349/1000\n",
      "3315/3315 [==============================] - 2s 473us/step - loss: 0.4089 - acc: 0.8594 - val_loss: 0.5335 - val_acc: 0.8200\n",
      "Epoch 350/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.4174 - acc: 0.8525 - val_loss: 0.5340 - val_acc: 0.8230\n",
      "Epoch 351/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.4206 - acc: 0.8543 - val_loss: 0.5357 - val_acc: 0.8163\n",
      "Epoch 352/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.4096 - acc: 0.8585 - val_loss: 0.5190 - val_acc: 0.8230\n",
      "Epoch 353/1000\n",
      "3315/3315 [==============================] - 2s 508us/step - loss: 0.4022 - acc: 0.8661 - val_loss: 0.5522 - val_acc: 0.8120\n",
      "Epoch 354/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.4105 - acc: 0.8582 - val_loss: 0.5183 - val_acc: 0.8328\n",
      "Epoch 355/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.4091 - acc: 0.8558 - val_loss: 0.5192 - val_acc: 0.8310\n",
      "Epoch 356/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.4025 - acc: 0.8679 - val_loss: 0.5163 - val_acc: 0.8224\n",
      "Epoch 357/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.4088 - acc: 0.8655 - val_loss: 0.5184 - val_acc: 0.8255\n",
      "Epoch 358/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.4061 - acc: 0.8591 - val_loss: 0.5139 - val_acc: 0.8371\n",
      "Epoch 359/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.4038 - acc: 0.8612 - val_loss: 0.5242 - val_acc: 0.8334\n",
      "Epoch 360/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.3973 - acc: 0.8643 - val_loss: 0.5438 - val_acc: 0.8181\n",
      "Epoch 361/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.4086 - acc: 0.8615 - val_loss: 0.5221 - val_acc: 0.8310\n",
      "Epoch 362/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.4103 - acc: 0.8489 - val_loss: 0.5063 - val_acc: 0.8310\n",
      "Epoch 363/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.3959 - acc: 0.8649 - val_loss: 0.5468 - val_acc: 0.8138\n",
      "Epoch 364/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.3940 - acc: 0.8652 - val_loss: 0.5313 - val_acc: 0.8230\n",
      "Epoch 365/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.3967 - acc: 0.8664 - val_loss: 0.5138 - val_acc: 0.8267\n",
      "Epoch 366/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.4046 - acc: 0.8637 - val_loss: 0.5209 - val_acc: 0.8236\n",
      "Epoch 367/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.3935 - acc: 0.8697 - val_loss: 0.5223 - val_acc: 0.8249\n",
      "Epoch 368/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.3943 - acc: 0.8676 - val_loss: 0.5134 - val_acc: 0.8249\n",
      "Epoch 369/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.3824 - acc: 0.8715 - val_loss: 0.5091 - val_acc: 0.8396\n",
      "Epoch 370/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.3841 - acc: 0.8682 - val_loss: 0.5189 - val_acc: 0.8236\n",
      "Epoch 371/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.3795 - acc: 0.8736 - val_loss: 0.5138 - val_acc: 0.8365\n",
      "Epoch 372/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.3957 - acc: 0.8649 - val_loss: 0.5514 - val_acc: 0.8187\n",
      "Epoch 373/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.3771 - acc: 0.8609 - val_loss: 0.4987 - val_acc: 0.8463\n",
      "Epoch 374/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.3864 - acc: 0.8630 - val_loss: 0.5121 - val_acc: 0.8291\n",
      "Epoch 375/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.3781 - acc: 0.8646 - val_loss: 0.4970 - val_acc: 0.8383\n",
      "Epoch 376/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.3855 - acc: 0.8627 - val_loss: 0.4975 - val_acc: 0.8371\n",
      "Epoch 377/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.3935 - acc: 0.8621 - val_loss: 0.5048 - val_acc: 0.8298\n",
      "Epoch 378/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.3802 - acc: 0.8670 - val_loss: 0.5144 - val_acc: 0.8194\n",
      "Epoch 379/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.3821 - acc: 0.8637 - val_loss: 0.5434 - val_acc: 0.8151\n",
      "Epoch 380/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.3801 - acc: 0.8627 - val_loss: 0.4913 - val_acc: 0.8500\n",
      "Epoch 381/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.3749 - acc: 0.8709 - val_loss: 0.5115 - val_acc: 0.8396\n",
      "Epoch 382/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.3799 - acc: 0.8694 - val_loss: 0.4924 - val_acc: 0.8438\n",
      "Epoch 383/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.3886 - acc: 0.8667 - val_loss: 0.5270 - val_acc: 0.8291\n",
      "Epoch 384/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.3633 - acc: 0.8658 - val_loss: 0.5808 - val_acc: 0.7991\n",
      "Epoch 385/1000\n",
      "3315/3315 [==============================] - 2s 479us/step - loss: 0.3757 - acc: 0.8679 - val_loss: 0.4922 - val_acc: 0.8469\n",
      "Epoch 386/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.3768 - acc: 0.8694 - val_loss: 0.4972 - val_acc: 0.8396\n",
      "Epoch 387/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.3772 - acc: 0.8676 - val_loss: 0.5041 - val_acc: 0.8353\n",
      "Epoch 388/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.3686 - acc: 0.8739 - val_loss: 0.4918 - val_acc: 0.8445\n",
      "Epoch 389/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.3780 - acc: 0.8664 - val_loss: 0.4958 - val_acc: 0.8426\n",
      "Epoch 390/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.3770 - acc: 0.8685 - val_loss: 0.4874 - val_acc: 0.8377\n",
      "Epoch 391/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.3541 - acc: 0.8724 - val_loss: 0.4819 - val_acc: 0.8457\n",
      "Epoch 392/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.3628 - acc: 0.8733 - val_loss: 0.4847 - val_acc: 0.8469\n",
      "Epoch 393/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.3754 - acc: 0.8706 - val_loss: 0.4920 - val_acc: 0.8420\n",
      "Epoch 394/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.3577 - acc: 0.8682 - val_loss: 0.5246 - val_acc: 0.8255\n",
      "Epoch 395/1000\n",
      "3315/3315 [==============================] - 2s 518us/step - loss: 0.3647 - acc: 0.8751 - val_loss: 0.4886 - val_acc: 0.8353\n",
      "Epoch 396/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.3732 - acc: 0.8697 - val_loss: 0.5019 - val_acc: 0.8334\n",
      "Epoch 397/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.3531 - acc: 0.8824 - val_loss: 0.4881 - val_acc: 0.8389\n",
      "Epoch 398/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.3457 - acc: 0.8824 - val_loss: 0.4807 - val_acc: 0.8475\n",
      "Epoch 399/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.3600 - acc: 0.8757 - val_loss: 0.4879 - val_acc: 0.8457\n",
      "Epoch 400/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.3567 - acc: 0.8760 - val_loss: 0.4980 - val_acc: 0.8273\n",
      "Epoch 401/1000\n",
      "3315/3315 [==============================] - 2s 512us/step - loss: 0.3587 - acc: 0.8784 - val_loss: 0.4719 - val_acc: 0.8438\n",
      "Epoch 402/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.3542 - acc: 0.8745 - val_loss: 0.4885 - val_acc: 0.8396\n",
      "Epoch 403/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.3716 - acc: 0.8733 - val_loss: 0.4734 - val_acc: 0.8445\n",
      "Epoch 404/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.3553 - acc: 0.8730 - val_loss: 0.4799 - val_acc: 0.8445\n",
      "Epoch 405/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.3475 - acc: 0.8787 - val_loss: 0.4849 - val_acc: 0.8408\n",
      "Epoch 406/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.3526 - acc: 0.8769 - val_loss: 0.5012 - val_acc: 0.8298\n",
      "Epoch 407/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.3450 - acc: 0.8842 - val_loss: 0.5160 - val_acc: 0.8267\n",
      "Epoch 408/1000\n",
      "3315/3315 [==============================] - 2s 476us/step - loss: 0.3566 - acc: 0.8778 - val_loss: 0.4881 - val_acc: 0.8371\n",
      "Epoch 409/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.3520 - acc: 0.8802 - val_loss: 0.4746 - val_acc: 0.8420\n",
      "Epoch 410/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.3566 - acc: 0.8775 - val_loss: 0.4747 - val_acc: 0.8457\n",
      "Epoch 411/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.3400 - acc: 0.8830 - val_loss: 0.4775 - val_acc: 0.8389\n",
      "Epoch 412/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.3490 - acc: 0.8827 - val_loss: 0.4763 - val_acc: 0.8481\n",
      "Epoch 413/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.3389 - acc: 0.8833 - val_loss: 0.4682 - val_acc: 0.8487\n",
      "Epoch 414/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.3593 - acc: 0.8718 - val_loss: 0.4868 - val_acc: 0.8426\n",
      "Epoch 415/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.3536 - acc: 0.8757 - val_loss: 0.4786 - val_acc: 0.8432\n",
      "Epoch 416/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.3401 - acc: 0.8860 - val_loss: 0.4799 - val_acc: 0.8371\n",
      "Epoch 417/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.3461 - acc: 0.8817 - val_loss: 0.4623 - val_acc: 0.8530\n",
      "Epoch 418/1000\n",
      "3315/3315 [==============================] - 2s 477us/step - loss: 0.3434 - acc: 0.8817 - val_loss: 0.4640 - val_acc: 0.8579\n",
      "Epoch 419/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.3418 - acc: 0.8811 - val_loss: 0.4600 - val_acc: 0.8518\n",
      "Epoch 420/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.3316 - acc: 0.8899 - val_loss: 0.4773 - val_acc: 0.8377\n",
      "Epoch 421/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.3387 - acc: 0.8893 - val_loss: 0.4652 - val_acc: 0.8549\n",
      "Epoch 422/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.3329 - acc: 0.8851 - val_loss: 0.4554 - val_acc: 0.8579\n",
      "Epoch 423/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.3407 - acc: 0.8821 - val_loss: 0.4815 - val_acc: 0.8408\n",
      "Epoch 424/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.3242 - acc: 0.8938 - val_loss: 0.4724 - val_acc: 0.8481\n",
      "Epoch 425/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.3507 - acc: 0.8757 - val_loss: 0.4715 - val_acc: 0.8475\n",
      "Epoch 426/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.3433 - acc: 0.8817 - val_loss: 0.4631 - val_acc: 0.8585\n",
      "Epoch 427/1000\n",
      "3315/3315 [==============================] - 2s 509us/step - loss: 0.3358 - acc: 0.8869 - val_loss: 0.4627 - val_acc: 0.8579\n",
      "Epoch 428/1000\n",
      "3315/3315 [==============================] - 2s 520us/step - loss: 0.3297 - acc: 0.8869 - val_loss: 0.4734 - val_acc: 0.8408\n",
      "Epoch 429/1000\n",
      "3315/3315 [==============================] - 2s 515us/step - loss: 0.3423 - acc: 0.8839 - val_loss: 0.4524 - val_acc: 0.8500\n",
      "Epoch 430/1000\n",
      "3315/3315 [==============================] - 2s 512us/step - loss: 0.3491 - acc: 0.8905 - val_loss: 0.4596 - val_acc: 0.8549\n",
      "Epoch 431/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.3199 - acc: 0.8947 - val_loss: 0.4560 - val_acc: 0.8487\n",
      "Epoch 432/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.3456 - acc: 0.8821 - val_loss: 0.4334 - val_acc: 0.8677\n",
      "Epoch 433/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.3226 - acc: 0.8890 - val_loss: 0.4600 - val_acc: 0.8487\n",
      "Epoch 434/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 0.3216 - acc: 0.8953 - val_loss: 0.4518 - val_acc: 0.8604\n",
      "Epoch 435/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.3306 - acc: 0.8833 - val_loss: 0.4793 - val_acc: 0.8420\n",
      "Epoch 436/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.3405 - acc: 0.8817 - val_loss: 0.4654 - val_acc: 0.8506\n",
      "Epoch 437/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.3342 - acc: 0.8890 - val_loss: 0.4699 - val_acc: 0.8494\n",
      "Epoch 438/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.3179 - acc: 0.8917 - val_loss: 0.4456 - val_acc: 0.8653\n",
      "Epoch 439/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.3312 - acc: 0.8848 - val_loss: 0.4591 - val_acc: 0.8518\n",
      "Epoch 440/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.3294 - acc: 0.8811 - val_loss: 0.4562 - val_acc: 0.8579\n",
      "Epoch 441/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.3248 - acc: 0.8854 - val_loss: 0.4506 - val_acc: 0.8555\n",
      "Epoch 442/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.3321 - acc: 0.8851 - val_loss: 0.4373 - val_acc: 0.8659\n",
      "Epoch 443/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.3304 - acc: 0.8872 - val_loss: 0.4438 - val_acc: 0.8518\n",
      "Epoch 444/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.3098 - acc: 0.8890 - val_loss: 0.4821 - val_acc: 0.8340\n",
      "Epoch 445/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.3202 - acc: 0.8944 - val_loss: 0.5176 - val_acc: 0.8218\n",
      "Epoch 446/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.3082 - acc: 0.8995 - val_loss: 0.4574 - val_acc: 0.8469\n",
      "Epoch 447/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.3159 - acc: 0.8896 - val_loss: 0.4863 - val_acc: 0.8389\n",
      "Epoch 448/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.3157 - acc: 0.8914 - val_loss: 0.4588 - val_acc: 0.8494\n",
      "Epoch 449/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.3123 - acc: 0.8863 - val_loss: 0.4413 - val_acc: 0.8622\n",
      "Epoch 450/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.3184 - acc: 0.8878 - val_loss: 0.4579 - val_acc: 0.8494\n",
      "Epoch 451/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.3099 - acc: 0.8917 - val_loss: 0.4384 - val_acc: 0.8555\n",
      "Epoch 452/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.3172 - acc: 0.8920 - val_loss: 0.4693 - val_acc: 0.8506\n",
      "Epoch 453/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.3076 - acc: 0.8956 - val_loss: 0.4506 - val_acc: 0.8530\n",
      "Epoch 454/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.3038 - acc: 0.8938 - val_loss: 0.4684 - val_acc: 0.8506\n",
      "Epoch 455/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.3122 - acc: 0.8950 - val_loss: 0.4431 - val_acc: 0.8585\n",
      "Epoch 456/1000\n",
      "3315/3315 [==============================] - 2s 508us/step - loss: 0.3190 - acc: 0.8896 - val_loss: 0.4408 - val_acc: 0.8641\n",
      "Epoch 457/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.3074 - acc: 0.8950 - val_loss: 0.4500 - val_acc: 0.8512\n",
      "Epoch 458/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.3026 - acc: 0.8971 - val_loss: 0.4360 - val_acc: 0.8481\n",
      "Epoch 459/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.3125 - acc: 0.8929 - val_loss: 0.4551 - val_acc: 0.8616\n",
      "Epoch 460/1000\n",
      "3315/3315 [==============================] - 2s 473us/step - loss: 0.2921 - acc: 0.9017 - val_loss: 0.4490 - val_acc: 0.8530\n",
      "Epoch 461/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.3157 - acc: 0.8905 - val_loss: 0.4434 - val_acc: 0.8671\n",
      "Epoch 462/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.3273 - acc: 0.8905 - val_loss: 0.4680 - val_acc: 0.8524\n",
      "Epoch 463/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.3109 - acc: 0.8902 - val_loss: 0.4492 - val_acc: 0.8524\n",
      "Epoch 464/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2876 - acc: 0.9041 - val_loss: 0.4327 - val_acc: 0.8585\n",
      "Epoch 465/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.3066 - acc: 0.8956 - val_loss: 0.4436 - val_acc: 0.8653\n",
      "Epoch 466/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 0.2997 - acc: 0.9050 - val_loss: 0.4474 - val_acc: 0.8500\n",
      "Epoch 467/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.3117 - acc: 0.8908 - val_loss: 0.4518 - val_acc: 0.8481\n",
      "Epoch 468/1000\n",
      "3315/3315 [==============================] - 2s 515us/step - loss: 0.2992 - acc: 0.8992 - val_loss: 0.4268 - val_acc: 0.8628\n",
      "Epoch 469/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.2922 - acc: 0.9002 - val_loss: 0.4822 - val_acc: 0.8396\n",
      "Epoch 470/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.3092 - acc: 0.8953 - val_loss: 0.4435 - val_acc: 0.8549\n",
      "Epoch 471/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.3143 - acc: 0.8983 - val_loss: 0.4541 - val_acc: 0.8604\n",
      "Epoch 472/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.3121 - acc: 0.8932 - val_loss: 0.4238 - val_acc: 0.8665\n",
      "Epoch 473/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.3101 - acc: 0.8890 - val_loss: 0.4198 - val_acc: 0.8659\n",
      "Epoch 474/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2993 - acc: 0.8983 - val_loss: 0.4261 - val_acc: 0.8647\n",
      "Epoch 475/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.2966 - acc: 0.9038 - val_loss: 0.4331 - val_acc: 0.8641\n",
      "Epoch 476/1000\n",
      "3315/3315 [==============================] - 2s 509us/step - loss: 0.3017 - acc: 0.8932 - val_loss: 0.4425 - val_acc: 0.8628\n",
      "Epoch 477/1000\n",
      "3315/3315 [==============================] - 2s 514us/step - loss: 0.2869 - acc: 0.8971 - val_loss: 0.4611 - val_acc: 0.8481\n",
      "Epoch 478/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.2943 - acc: 0.9050 - val_loss: 0.4336 - val_acc: 0.8726\n",
      "Epoch 479/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2971 - acc: 0.8929 - val_loss: 0.4400 - val_acc: 0.8561\n",
      "Epoch 480/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2868 - acc: 0.9017 - val_loss: 0.4330 - val_acc: 0.8622\n",
      "Epoch 481/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2955 - acc: 0.8956 - val_loss: 0.4277 - val_acc: 0.8702\n",
      "Epoch 482/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.2893 - acc: 0.8974 - val_loss: 0.4340 - val_acc: 0.8622\n",
      "Epoch 483/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.2839 - acc: 0.9017 - val_loss: 0.4063 - val_acc: 0.8683\n",
      "Epoch 484/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.2838 - acc: 0.9014 - val_loss: 0.4270 - val_acc: 0.8610\n",
      "Epoch 485/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.2878 - acc: 0.9071 - val_loss: 0.4525 - val_acc: 0.8506\n",
      "Epoch 486/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.2958 - acc: 0.8989 - val_loss: 0.4455 - val_acc: 0.8573\n",
      "Epoch 487/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.2905 - acc: 0.9029 - val_loss: 0.4218 - val_acc: 0.8781\n",
      "Epoch 488/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.2876 - acc: 0.9032 - val_loss: 0.4400 - val_acc: 0.8555\n",
      "Epoch 489/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.2961 - acc: 0.8995 - val_loss: 0.4403 - val_acc: 0.8622\n",
      "Epoch 490/1000\n",
      "3315/3315 [==============================] - 2s 514us/step - loss: 0.2907 - acc: 0.9041 - val_loss: 0.4361 - val_acc: 0.8622\n",
      "Epoch 491/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.2789 - acc: 0.9074 - val_loss: 0.4161 - val_acc: 0.8653\n",
      "Epoch 492/1000\n",
      "3315/3315 [==============================] - 2s 550us/step - loss: 0.2808 - acc: 0.9014 - val_loss: 0.4588 - val_acc: 0.8585\n",
      "Epoch 493/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.2768 - acc: 0.9056 - val_loss: 0.4263 - val_acc: 0.8634\n",
      "Epoch 494/1000\n",
      "3315/3315 [==============================] - 2s 561us/step - loss: 0.2929 - acc: 0.8947 - val_loss: 0.4280 - val_acc: 0.8610\n",
      "Epoch 495/1000\n",
      "3315/3315 [==============================] - 2s 548us/step - loss: 0.2978 - acc: 0.8950 - val_loss: 0.4254 - val_acc: 0.8653\n",
      "Epoch 496/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 0.2872 - acc: 0.9014 - val_loss: 0.4288 - val_acc: 0.8677\n",
      "Epoch 497/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.2847 - acc: 0.9008 - val_loss: 0.4174 - val_acc: 0.8708\n",
      "Epoch 498/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.2833 - acc: 0.9032 - val_loss: 0.4263 - val_acc: 0.8567\n",
      "Epoch 499/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 0.2845 - acc: 0.9035 - val_loss: 0.4487 - val_acc: 0.8579\n",
      "Epoch 500/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.2756 - acc: 0.9053 - val_loss: 0.4069 - val_acc: 0.8702\n",
      "Epoch 501/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.2735 - acc: 0.9095 - val_loss: 0.4455 - val_acc: 0.8543\n",
      "Epoch 502/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 0.2871 - acc: 0.9008 - val_loss: 0.4056 - val_acc: 0.8788\n",
      "Epoch 503/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.2799 - acc: 0.9017 - val_loss: 0.4244 - val_acc: 0.8616\n",
      "Epoch 504/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.2789 - acc: 0.9083 - val_loss: 0.4262 - val_acc: 0.8610\n",
      "Epoch 505/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.2764 - acc: 0.9065 - val_loss: 0.4218 - val_acc: 0.8628\n",
      "Epoch 506/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2721 - acc: 0.9140 - val_loss: 0.4807 - val_acc: 0.8420\n",
      "Epoch 507/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2843 - acc: 0.9092 - val_loss: 0.4342 - val_acc: 0.8567\n",
      "Epoch 508/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.2818 - acc: 0.9020 - val_loss: 0.4401 - val_acc: 0.8567\n",
      "Epoch 509/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.2907 - acc: 0.9014 - val_loss: 0.4027 - val_acc: 0.8708\n",
      "Epoch 510/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2737 - acc: 0.9080 - val_loss: 0.4238 - val_acc: 0.8616\n",
      "Epoch 511/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.2762 - acc: 0.9008 - val_loss: 0.4262 - val_acc: 0.8677\n",
      "Epoch 512/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.2763 - acc: 0.9065 - val_loss: 0.4163 - val_acc: 0.8628\n",
      "Epoch 513/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2823 - acc: 0.9017 - val_loss: 0.4154 - val_acc: 0.8714\n",
      "Epoch 514/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.2831 - acc: 0.9086 - val_loss: 0.4188 - val_acc: 0.8671\n",
      "Epoch 515/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2784 - acc: 0.9005 - val_loss: 0.4068 - val_acc: 0.8739\n",
      "Epoch 516/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2675 - acc: 0.9074 - val_loss: 0.4116 - val_acc: 0.8634\n",
      "Epoch 517/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.2927 - acc: 0.8980 - val_loss: 0.4175 - val_acc: 0.8604\n",
      "Epoch 518/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.2724 - acc: 0.9062 - val_loss: 0.4241 - val_acc: 0.8732\n",
      "Epoch 519/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.2591 - acc: 0.9122 - val_loss: 0.4463 - val_acc: 0.8506\n",
      "Epoch 520/1000\n",
      "3315/3315 [==============================] - 2s 572us/step - loss: 0.2715 - acc: 0.9044 - val_loss: 0.4074 - val_acc: 0.8757\n",
      "Epoch 521/1000\n",
      "3315/3315 [==============================] - 2s 565us/step - loss: 0.2697 - acc: 0.9104 - val_loss: 0.3985 - val_acc: 0.8806\n",
      "Epoch 522/1000\n",
      "3315/3315 [==============================] - 2s 557us/step - loss: 0.2725 - acc: 0.9041 - val_loss: 0.4251 - val_acc: 0.8690\n",
      "Epoch 523/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2728 - acc: 0.9050 - val_loss: 0.4265 - val_acc: 0.8665\n",
      "Epoch 524/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.2670 - acc: 0.9071 - val_loss: 0.4266 - val_acc: 0.8659\n",
      "Epoch 525/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.2750 - acc: 0.9029 - val_loss: 0.4095 - val_acc: 0.8732\n",
      "Epoch 526/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.2583 - acc: 0.9119 - val_loss: 0.4091 - val_acc: 0.8726\n",
      "Epoch 527/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.2750 - acc: 0.9083 - val_loss: 0.4334 - val_acc: 0.8524\n",
      "Epoch 528/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.2633 - acc: 0.9146 - val_loss: 0.4138 - val_acc: 0.8739\n",
      "Epoch 529/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.2653 - acc: 0.9068 - val_loss: 0.3844 - val_acc: 0.8836\n",
      "Epoch 530/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.2681 - acc: 0.9080 - val_loss: 0.4235 - val_acc: 0.8696\n",
      "Epoch 531/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.2656 - acc: 0.9077 - val_loss: 0.4089 - val_acc: 0.8781\n",
      "Epoch 532/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.2562 - acc: 0.9083 - val_loss: 0.4056 - val_acc: 0.8726\n",
      "Epoch 533/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2603 - acc: 0.9101 - val_loss: 0.4016 - val_acc: 0.8800\n",
      "Epoch 534/1000\n",
      "3315/3315 [==============================] - 2s 477us/step - loss: 0.2632 - acc: 0.9071 - val_loss: 0.3982 - val_acc: 0.8739\n",
      "Epoch 535/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.2511 - acc: 0.9137 - val_loss: 0.4029 - val_acc: 0.8732\n",
      "Epoch 536/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.2517 - acc: 0.9170 - val_loss: 0.4098 - val_acc: 0.8665\n",
      "Epoch 537/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.2567 - acc: 0.9137 - val_loss: 0.4030 - val_acc: 0.8757\n",
      "Epoch 538/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.2489 - acc: 0.9158 - val_loss: 0.3926 - val_acc: 0.8843\n",
      "Epoch 539/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.2603 - acc: 0.9077 - val_loss: 0.4224 - val_acc: 0.8628\n",
      "Epoch 540/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.2592 - acc: 0.9059 - val_loss: 0.4073 - val_acc: 0.8757\n",
      "Epoch 541/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.2716 - acc: 0.9089 - val_loss: 0.4121 - val_acc: 0.8763\n",
      "Epoch 542/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.2528 - acc: 0.9101 - val_loss: 0.4083 - val_acc: 0.8708\n",
      "Epoch 543/1000\n",
      "3315/3315 [==============================] - 2s 519us/step - loss: 0.2598 - acc: 0.9116 - val_loss: 0.4073 - val_acc: 0.8702\n",
      "Epoch 544/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.2608 - acc: 0.9074 - val_loss: 0.4078 - val_acc: 0.8720\n",
      "Epoch 545/1000\n",
      "3315/3315 [==============================] - 2s 509us/step - loss: 0.2591 - acc: 0.9074 - val_loss: 0.4070 - val_acc: 0.8739\n",
      "Epoch 546/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.2536 - acc: 0.9158 - val_loss: 0.4058 - val_acc: 0.8745\n",
      "Epoch 547/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.2454 - acc: 0.9167 - val_loss: 0.4169 - val_acc: 0.8720\n",
      "Epoch 548/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.2470 - acc: 0.9152 - val_loss: 0.4053 - val_acc: 0.8757\n",
      "Epoch 549/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.2557 - acc: 0.9158 - val_loss: 0.4038 - val_acc: 0.8757\n",
      "Epoch 550/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.2577 - acc: 0.9089 - val_loss: 0.4155 - val_acc: 0.8616\n",
      "Epoch 551/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.2498 - acc: 0.9146 - val_loss: 0.4118 - val_acc: 0.8720\n",
      "Epoch 552/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.2379 - acc: 0.9201 - val_loss: 0.3916 - val_acc: 0.8794\n",
      "Epoch 553/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.2540 - acc: 0.9167 - val_loss: 0.3987 - val_acc: 0.8739\n",
      "Epoch 554/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.2520 - acc: 0.9068 - val_loss: 0.4007 - val_acc: 0.8800\n",
      "Epoch 555/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.2495 - acc: 0.9098 - val_loss: 0.3905 - val_acc: 0.8843\n",
      "Epoch 556/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.2524 - acc: 0.9128 - val_loss: 0.3999 - val_acc: 0.8757\n",
      "Epoch 557/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.2638 - acc: 0.9104 - val_loss: 0.4129 - val_acc: 0.8726\n",
      "Epoch 558/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.2494 - acc: 0.9119 - val_loss: 0.3941 - val_acc: 0.8861\n",
      "Epoch 559/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.2439 - acc: 0.9161 - val_loss: 0.3951 - val_acc: 0.8800\n",
      "Epoch 560/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.2309 - acc: 0.9216 - val_loss: 0.4012 - val_acc: 0.8788\n",
      "Epoch 561/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.2432 - acc: 0.9189 - val_loss: 0.3896 - val_acc: 0.8763\n",
      "Epoch 562/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.2512 - acc: 0.9189 - val_loss: 0.4465 - val_acc: 0.8598\n",
      "Epoch 563/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.2666 - acc: 0.9080 - val_loss: 0.4111 - val_acc: 0.8806\n",
      "Epoch 564/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2315 - acc: 0.9222 - val_loss: 0.3877 - val_acc: 0.8788\n",
      "Epoch 565/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.2474 - acc: 0.9161 - val_loss: 0.3917 - val_acc: 0.8769\n",
      "Epoch 566/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.2516 - acc: 0.9137 - val_loss: 0.4064 - val_acc: 0.8739\n",
      "Epoch 567/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.2327 - acc: 0.9243 - val_loss: 0.3802 - val_acc: 0.8843\n",
      "Epoch 568/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.2425 - acc: 0.9186 - val_loss: 0.3964 - val_acc: 0.8775\n",
      "Epoch 569/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.2356 - acc: 0.9201 - val_loss: 0.4246 - val_acc: 0.8616\n",
      "Epoch 570/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.2406 - acc: 0.9155 - val_loss: 0.3933 - val_acc: 0.8751\n",
      "Epoch 571/1000\n",
      "3315/3315 [==============================] - 2s 514us/step - loss: 0.2370 - acc: 0.9237 - val_loss: 0.3891 - val_acc: 0.8812\n",
      "Epoch 572/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.2274 - acc: 0.9219 - val_loss: 0.4013 - val_acc: 0.8830\n",
      "Epoch 573/1000\n",
      "3315/3315 [==============================] - 2s 523us/step - loss: 0.2440 - acc: 0.9179 - val_loss: 0.3827 - val_acc: 0.8885\n",
      "Epoch 574/1000\n",
      "3315/3315 [==============================] - 2s 515us/step - loss: 0.2267 - acc: 0.9176 - val_loss: 0.4044 - val_acc: 0.8781\n",
      "Epoch 575/1000\n",
      "3315/3315 [==============================] - 2s 519us/step - loss: 0.2500 - acc: 0.9198 - val_loss: 0.3908 - val_acc: 0.8836\n",
      "Epoch 576/1000\n",
      "3315/3315 [==============================] - 2s 517us/step - loss: 0.2312 - acc: 0.9258 - val_loss: 0.3938 - val_acc: 0.8726\n",
      "Epoch 577/1000\n",
      "3315/3315 [==============================] - 2s 522us/step - loss: 0.2457 - acc: 0.9204 - val_loss: 0.3967 - val_acc: 0.8775\n",
      "Epoch 578/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.2311 - acc: 0.9234 - val_loss: 0.4225 - val_acc: 0.8641\n",
      "Epoch 579/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.2415 - acc: 0.9204 - val_loss: 0.3856 - val_acc: 0.8788\n",
      "Epoch 580/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.2448 - acc: 0.9207 - val_loss: 0.3861 - val_acc: 0.8824\n",
      "Epoch 581/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.2317 - acc: 0.9276 - val_loss: 0.3821 - val_acc: 0.8855\n",
      "Epoch 582/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.2349 - acc: 0.9149 - val_loss: 0.3825 - val_acc: 0.8800\n",
      "Epoch 583/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.2235 - acc: 0.9219 - val_loss: 0.3944 - val_acc: 0.8806\n",
      "Epoch 584/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.2384 - acc: 0.9213 - val_loss: 0.3960 - val_acc: 0.8916\n",
      "Epoch 585/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.2265 - acc: 0.9222 - val_loss: 0.3943 - val_acc: 0.8757\n",
      "Epoch 586/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.2278 - acc: 0.9264 - val_loss: 0.4347 - val_acc: 0.8610\n",
      "Epoch 587/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.2446 - acc: 0.9155 - val_loss: 0.3895 - val_acc: 0.8794\n",
      "Epoch 588/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.2247 - acc: 0.9258 - val_loss: 0.3951 - val_acc: 0.8843\n",
      "Epoch 589/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.2375 - acc: 0.9189 - val_loss: 0.3934 - val_acc: 0.8800\n",
      "Epoch 590/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.2370 - acc: 0.9170 - val_loss: 0.3921 - val_acc: 0.8812\n",
      "Epoch 591/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.2339 - acc: 0.9186 - val_loss: 0.4168 - val_acc: 0.8628\n",
      "Epoch 592/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.2411 - acc: 0.9216 - val_loss: 0.3685 - val_acc: 0.8892\n",
      "Epoch 593/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.2406 - acc: 0.9131 - val_loss: 0.3981 - val_acc: 0.8683\n",
      "Epoch 594/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.2246 - acc: 0.9231 - val_loss: 0.3883 - val_acc: 0.8898\n",
      "Epoch 595/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.2172 - acc: 0.9249 - val_loss: 0.3832 - val_acc: 0.8824\n",
      "Epoch 596/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2284 - acc: 0.9195 - val_loss: 0.3995 - val_acc: 0.8830\n",
      "Epoch 597/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.2340 - acc: 0.9198 - val_loss: 0.3832 - val_acc: 0.8849\n",
      "Epoch 598/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.2236 - acc: 0.9234 - val_loss: 0.4012 - val_acc: 0.8904\n",
      "Epoch 599/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2131 - acc: 0.9294 - val_loss: 0.3789 - val_acc: 0.8836\n",
      "Epoch 600/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.2322 - acc: 0.9176 - val_loss: 0.3925 - val_acc: 0.8757\n",
      "Epoch 601/1000\n",
      "3315/3315 [==============================] - 2s 515us/step - loss: 0.2217 - acc: 0.9240 - val_loss: 0.3846 - val_acc: 0.8806\n",
      "Epoch 602/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.2209 - acc: 0.9270 - val_loss: 0.3916 - val_acc: 0.8861\n",
      "Epoch 603/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.2204 - acc: 0.9264 - val_loss: 0.3813 - val_acc: 0.8843\n",
      "Epoch 604/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.2191 - acc: 0.9276 - val_loss: 0.3796 - val_acc: 0.8824\n",
      "Epoch 605/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2295 - acc: 0.9192 - val_loss: 0.3867 - val_acc: 0.8812\n",
      "Epoch 606/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.2253 - acc: 0.9231 - val_loss: 0.3831 - val_acc: 0.8855\n",
      "Epoch 607/1000\n",
      "3315/3315 [==============================] - 2s 510us/step - loss: 0.2269 - acc: 0.9228 - val_loss: 0.3751 - val_acc: 0.8892\n",
      "Epoch 608/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.2281 - acc: 0.9219 - val_loss: 0.3955 - val_acc: 0.8794\n",
      "Epoch 609/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.2266 - acc: 0.9195 - val_loss: 0.3711 - val_acc: 0.8910\n",
      "Epoch 610/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.2273 - acc: 0.9252 - val_loss: 0.3714 - val_acc: 0.8934\n",
      "Epoch 611/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.2176 - acc: 0.9210 - val_loss: 0.4027 - val_acc: 0.8739\n",
      "Epoch 612/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 0.2119 - acc: 0.9294 - val_loss: 0.4004 - val_acc: 0.8806\n",
      "Epoch 613/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.2143 - acc: 0.9291 - val_loss: 0.3855 - val_acc: 0.8910\n",
      "Epoch 614/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2281 - acc: 0.9222 - val_loss: 0.3746 - val_acc: 0.8898\n",
      "Epoch 615/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2303 - acc: 0.9192 - val_loss: 0.3838 - val_acc: 0.8824\n",
      "Epoch 616/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.2214 - acc: 0.9228 - val_loss: 0.3649 - val_acc: 0.8959\n",
      "Epoch 617/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2145 - acc: 0.9222 - val_loss: 0.3665 - val_acc: 0.8910\n",
      "Epoch 618/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2106 - acc: 0.9240 - val_loss: 0.3950 - val_acc: 0.8843\n",
      "Epoch 619/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.2278 - acc: 0.9173 - val_loss: 0.3824 - val_acc: 0.8910\n",
      "Epoch 620/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.2295 - acc: 0.9189 - val_loss: 0.3825 - val_acc: 0.8861\n",
      "Epoch 621/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.2263 - acc: 0.9231 - val_loss: 0.3863 - val_acc: 0.8849\n",
      "Epoch 622/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.2315 - acc: 0.9204 - val_loss: 0.3814 - val_acc: 0.8818\n",
      "Epoch 623/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.2207 - acc: 0.9210 - val_loss: 0.3815 - val_acc: 0.8873\n",
      "Epoch 624/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.2124 - acc: 0.9351 - val_loss: 0.3593 - val_acc: 0.8922\n",
      "Epoch 625/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.2116 - acc: 0.9237 - val_loss: 0.3837 - val_acc: 0.8867\n",
      "Epoch 626/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.2105 - acc: 0.9264 - val_loss: 0.3785 - val_acc: 0.8830\n",
      "Epoch 627/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2232 - acc: 0.9264 - val_loss: 0.3675 - val_acc: 0.8910\n",
      "Epoch 628/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.2132 - acc: 0.9243 - val_loss: 0.3767 - val_acc: 0.8898\n",
      "Epoch 629/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.2199 - acc: 0.9282 - val_loss: 0.3738 - val_acc: 0.8947\n",
      "Epoch 630/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.2124 - acc: 0.9267 - val_loss: 0.3620 - val_acc: 0.8934\n",
      "Epoch 631/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.2249 - acc: 0.9228 - val_loss: 0.3641 - val_acc: 0.8965\n",
      "Epoch 632/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.2116 - acc: 0.9264 - val_loss: 0.3950 - val_acc: 0.8836\n",
      "Epoch 633/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.2092 - acc: 0.9315 - val_loss: 0.3766 - val_acc: 0.8867\n",
      "Epoch 634/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.2201 - acc: 0.9321 - val_loss: 0.3717 - val_acc: 0.8910\n",
      "Epoch 635/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.2205 - acc: 0.9228 - val_loss: 0.3866 - val_acc: 0.8898\n",
      "Epoch 636/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1986 - acc: 0.9297 - val_loss: 0.3725 - val_acc: 0.8910\n",
      "Epoch 637/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.2146 - acc: 0.9264 - val_loss: 0.3848 - val_acc: 0.8818\n",
      "Epoch 638/1000\n",
      "3315/3315 [==============================] - 2s 520us/step - loss: 0.2129 - acc: 0.9312 - val_loss: 0.3971 - val_acc: 0.8788\n",
      "Epoch 639/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 0.2063 - acc: 0.9276 - val_loss: 0.3693 - val_acc: 0.8898\n",
      "Epoch 640/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 0.2101 - acc: 0.9306 - val_loss: 0.3674 - val_acc: 0.8916\n",
      "Epoch 641/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.2173 - acc: 0.9246 - val_loss: 0.3764 - val_acc: 0.8867\n",
      "Epoch 642/1000\n",
      "3315/3315 [==============================] - 2s 515us/step - loss: 0.2144 - acc: 0.9297 - val_loss: 0.3727 - val_acc: 0.8965\n",
      "Epoch 643/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.2118 - acc: 0.9282 - val_loss: 0.3812 - val_acc: 0.8934\n",
      "Epoch 644/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.2016 - acc: 0.9330 - val_loss: 0.3980 - val_acc: 0.8861\n",
      "Epoch 645/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.2049 - acc: 0.9315 - val_loss: 0.3736 - val_acc: 0.8843\n",
      "Epoch 646/1000\n",
      "3315/3315 [==============================] - 2s 508us/step - loss: 0.2034 - acc: 0.9330 - val_loss: 0.3706 - val_acc: 0.8879\n",
      "Epoch 647/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.1960 - acc: 0.9370 - val_loss: 0.3917 - val_acc: 0.8855\n",
      "Epoch 648/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.2083 - acc: 0.9246 - val_loss: 0.3734 - val_acc: 0.8928\n",
      "Epoch 649/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.2048 - acc: 0.9276 - val_loss: 0.3585 - val_acc: 0.8965\n",
      "Epoch 650/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.2188 - acc: 0.9270 - val_loss: 0.3827 - val_acc: 0.8861\n",
      "Epoch 651/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.2043 - acc: 0.9285 - val_loss: 0.3956 - val_acc: 0.8781\n",
      "Epoch 652/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.2032 - acc: 0.9294 - val_loss: 0.3791 - val_acc: 0.8910\n",
      "Epoch 653/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2094 - acc: 0.9297 - val_loss: 0.3674 - val_acc: 0.8941\n",
      "Epoch 654/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1972 - acc: 0.9327 - val_loss: 0.3705 - val_acc: 0.8953\n",
      "Epoch 655/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.2125 - acc: 0.9276 - val_loss: 0.3630 - val_acc: 0.8953\n",
      "Epoch 656/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.2028 - acc: 0.9330 - val_loss: 0.3754 - val_acc: 0.8892\n",
      "Epoch 657/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.2170 - acc: 0.9252 - val_loss: 0.3566 - val_acc: 0.8959\n",
      "Epoch 658/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.2121 - acc: 0.9327 - val_loss: 0.3719 - val_acc: 0.8959\n",
      "Epoch 659/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.2031 - acc: 0.9367 - val_loss: 0.3747 - val_acc: 0.8934\n",
      "Epoch 660/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.2038 - acc: 0.9321 - val_loss: 0.3716 - val_acc: 0.8892\n",
      "Epoch 661/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.2065 - acc: 0.9300 - val_loss: 0.3683 - val_acc: 0.8965\n",
      "Epoch 662/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.2167 - acc: 0.9264 - val_loss: 0.3739 - val_acc: 0.8971\n",
      "Epoch 663/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.2206 - acc: 0.9291 - val_loss: 0.3670 - val_acc: 0.8971\n",
      "Epoch 664/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.2010 - acc: 0.9324 - val_loss: 0.3844 - val_acc: 0.8806\n",
      "Epoch 665/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.1946 - acc: 0.9400 - val_loss: 0.3923 - val_acc: 0.8849\n",
      "Epoch 666/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.2078 - acc: 0.9228 - val_loss: 0.3645 - val_acc: 0.8941\n",
      "Epoch 667/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.2068 - acc: 0.9288 - val_loss: 0.3747 - val_acc: 0.8947\n",
      "Epoch 668/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.2088 - acc: 0.9276 - val_loss: 0.3843 - val_acc: 0.8879\n",
      "Epoch 669/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1994 - acc: 0.9339 - val_loss: 0.3740 - val_acc: 0.8916\n",
      "Epoch 670/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.2046 - acc: 0.9333 - val_loss: 0.3597 - val_acc: 0.8990\n",
      "Epoch 671/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.2096 - acc: 0.9249 - val_loss: 0.3815 - val_acc: 0.8922\n",
      "Epoch 672/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.1898 - acc: 0.9330 - val_loss: 0.3692 - val_acc: 0.8941\n",
      "Epoch 673/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1992 - acc: 0.9321 - val_loss: 0.3772 - val_acc: 0.8990\n",
      "Epoch 674/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.2054 - acc: 0.9285 - val_loss: 0.3670 - val_acc: 0.9002\n",
      "Epoch 675/1000\n",
      "3315/3315 [==============================] - 2s 509us/step - loss: 0.2022 - acc: 0.9348 - val_loss: 0.3739 - val_acc: 0.8959\n",
      "Epoch 676/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.2072 - acc: 0.9285 - val_loss: 0.3747 - val_acc: 0.8965\n",
      "Epoch 677/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1946 - acc: 0.9342 - val_loss: 0.3686 - val_acc: 0.8959\n",
      "Epoch 678/1000\n",
      "3315/3315 [==============================] - 2s 551us/step - loss: 0.2014 - acc: 0.9297 - val_loss: 0.3581 - val_acc: 0.9014\n",
      "Epoch 679/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.2052 - acc: 0.9261 - val_loss: 0.3590 - val_acc: 0.8959\n",
      "Epoch 680/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.2168 - acc: 0.9261 - val_loss: 0.3648 - val_acc: 0.8904\n",
      "Epoch 681/1000\n",
      "3315/3315 [==============================] - 2s 554us/step - loss: 0.1875 - acc: 0.9403 - val_loss: 0.3704 - val_acc: 0.8941\n",
      "Epoch 682/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.1883 - acc: 0.9354 - val_loss: 0.3620 - val_acc: 0.8916\n",
      "Epoch 683/1000\n",
      "3315/3315 [==============================] - 2s 549us/step - loss: 0.1931 - acc: 0.9324 - val_loss: 0.3588 - val_acc: 0.8892\n",
      "Epoch 684/1000\n",
      "3315/3315 [==============================] - 2s 517us/step - loss: 0.1858 - acc: 0.9379 - val_loss: 0.3658 - val_acc: 0.8953\n",
      "Epoch 685/1000\n",
      "3315/3315 [==============================] - 2s 513us/step - loss: 0.2008 - acc: 0.9336 - val_loss: 0.3562 - val_acc: 0.8916\n",
      "Epoch 686/1000\n",
      "3315/3315 [==============================] - 2s 516us/step - loss: 0.1850 - acc: 0.9342 - val_loss: 0.3680 - val_acc: 0.8953\n",
      "Epoch 687/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.1968 - acc: 0.9306 - val_loss: 0.3618 - val_acc: 0.8934\n",
      "Epoch 688/1000\n",
      "3315/3315 [==============================] - 2s 513us/step - loss: 0.1845 - acc: 0.9388 - val_loss: 0.3647 - val_acc: 0.8843\n",
      "Epoch 689/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.2001 - acc: 0.9285 - val_loss: 0.3750 - val_acc: 0.8934\n",
      "Epoch 690/1000\n",
      "3315/3315 [==============================] - 2s 512us/step - loss: 0.1896 - acc: 0.9363 - val_loss: 0.3532 - val_acc: 0.9020\n",
      "Epoch 691/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 0.1979 - acc: 0.9354 - val_loss: 0.3616 - val_acc: 0.8941\n",
      "Epoch 692/1000\n",
      "3315/3315 [==============================] - 2s 517us/step - loss: 0.1927 - acc: 0.9324 - val_loss: 0.3665 - val_acc: 0.8983\n",
      "Epoch 693/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.1893 - acc: 0.9351 - val_loss: 0.3836 - val_acc: 0.8898\n",
      "Epoch 694/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.1963 - acc: 0.9306 - val_loss: 0.3612 - val_acc: 0.8959\n",
      "Epoch 695/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1899 - acc: 0.9382 - val_loss: 0.3633 - val_acc: 0.8996\n",
      "Epoch 696/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1875 - acc: 0.9354 - val_loss: 0.3432 - val_acc: 0.8971\n",
      "Epoch 697/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.1991 - acc: 0.9306 - val_loss: 0.3476 - val_acc: 0.9014\n",
      "Epoch 698/1000\n",
      "3315/3315 [==============================] - 2s 508us/step - loss: 0.1889 - acc: 0.9376 - val_loss: 0.3717 - val_acc: 0.8965\n",
      "Epoch 699/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1875 - acc: 0.9382 - val_loss: 0.3676 - val_acc: 0.8953\n",
      "Epoch 700/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1750 - acc: 0.9412 - val_loss: 0.3564 - val_acc: 0.8953\n",
      "Epoch 701/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1866 - acc: 0.9309 - val_loss: 0.3947 - val_acc: 0.8873\n",
      "Epoch 702/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1877 - acc: 0.9360 - val_loss: 0.3584 - val_acc: 0.9026\n",
      "Epoch 703/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1887 - acc: 0.9354 - val_loss: 0.3675 - val_acc: 0.8959\n",
      "Epoch 704/1000\n",
      "3315/3315 [==============================] - 2s 544us/step - loss: 0.1852 - acc: 0.9363 - val_loss: 0.3669 - val_acc: 0.8928\n",
      "Epoch 705/1000\n",
      "3315/3315 [==============================] - 2s 540us/step - loss: 0.1823 - acc: 0.9315 - val_loss: 0.3534 - val_acc: 0.9045\n",
      "Epoch 706/1000\n",
      "3315/3315 [==============================] - 2s 565us/step - loss: 0.1775 - acc: 0.9409 - val_loss: 0.3879 - val_acc: 0.8861\n",
      "Epoch 707/1000\n",
      "3315/3315 [==============================] - 2s 514us/step - loss: 0.1975 - acc: 0.9312 - val_loss: 0.3537 - val_acc: 0.8934\n",
      "Epoch 708/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.1866 - acc: 0.9339 - val_loss: 0.3583 - val_acc: 0.9051\n",
      "Epoch 709/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1839 - acc: 0.9357 - val_loss: 0.3658 - val_acc: 0.8892\n",
      "Epoch 710/1000\n",
      "3315/3315 [==============================] - 2s 518us/step - loss: 0.1934 - acc: 0.9327 - val_loss: 0.3509 - val_acc: 0.9032\n",
      "Epoch 711/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1735 - acc: 0.9397 - val_loss: 0.3805 - val_acc: 0.8885\n",
      "Epoch 712/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1833 - acc: 0.9415 - val_loss: 0.3661 - val_acc: 0.9014\n",
      "Epoch 713/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1859 - acc: 0.9385 - val_loss: 0.3843 - val_acc: 0.8953\n",
      "Epoch 714/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1672 - acc: 0.9460 - val_loss: 0.3493 - val_acc: 0.8996\n",
      "Epoch 715/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1874 - acc: 0.9345 - val_loss: 0.3468 - val_acc: 0.8996\n",
      "Epoch 716/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1879 - acc: 0.9321 - val_loss: 0.3529 - val_acc: 0.8971\n",
      "Epoch 717/1000\n",
      "3315/3315 [==============================] - 2s 510us/step - loss: 0.1751 - acc: 0.9418 - val_loss: 0.3654 - val_acc: 0.8934\n",
      "Epoch 718/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.1840 - acc: 0.9363 - val_loss: 0.3566 - val_acc: 0.8971\n",
      "Epoch 719/1000\n",
      "3315/3315 [==============================] - 2s 508us/step - loss: 0.1897 - acc: 0.9394 - val_loss: 0.3634 - val_acc: 0.8959\n",
      "Epoch 720/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.1855 - acc: 0.9354 - val_loss: 0.3728 - val_acc: 0.8959\n",
      "Epoch 721/1000\n",
      "3315/3315 [==============================] - 2s 516us/step - loss: 0.1816 - acc: 0.9430 - val_loss: 0.3566 - val_acc: 0.9045\n",
      "Epoch 722/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1751 - acc: 0.9376 - val_loss: 0.3473 - val_acc: 0.9063\n",
      "Epoch 723/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1732 - acc: 0.9406 - val_loss: 0.3650 - val_acc: 0.8965\n",
      "Epoch 724/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.1870 - acc: 0.9379 - val_loss: 0.3522 - val_acc: 0.9032\n",
      "Epoch 725/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.2010 - acc: 0.9370 - val_loss: 0.3737 - val_acc: 0.8885\n",
      "Epoch 726/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.1753 - acc: 0.9439 - val_loss: 0.3674 - val_acc: 0.8965\n",
      "Epoch 727/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.1800 - acc: 0.9445 - val_loss: 0.3498 - val_acc: 0.9008\n",
      "Epoch 728/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.1686 - acc: 0.9460 - val_loss: 0.3595 - val_acc: 0.9026\n",
      "Epoch 729/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1645 - acc: 0.9448 - val_loss: 0.3577 - val_acc: 0.8947\n",
      "Epoch 730/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.1988 - acc: 0.9336 - val_loss: 0.3614 - val_acc: 0.8965\n",
      "Epoch 731/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.1788 - acc: 0.9373 - val_loss: 0.3573 - val_acc: 0.8941\n",
      "Epoch 732/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1832 - acc: 0.9409 - val_loss: 0.3522 - val_acc: 0.8953\n",
      "Epoch 733/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.1827 - acc: 0.9397 - val_loss: 0.3612 - val_acc: 0.9020\n",
      "Epoch 734/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.1872 - acc: 0.9376 - val_loss: 0.3590 - val_acc: 0.8996\n",
      "Epoch 735/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.1819 - acc: 0.9373 - val_loss: 0.3616 - val_acc: 0.8983\n",
      "Epoch 736/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1860 - acc: 0.9357 - val_loss: 0.3400 - val_acc: 0.9063\n",
      "Epoch 737/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.1781 - acc: 0.9427 - val_loss: 0.3508 - val_acc: 0.9020\n",
      "Epoch 738/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1666 - acc: 0.9439 - val_loss: 0.3573 - val_acc: 0.8983\n",
      "Epoch 739/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1799 - acc: 0.9379 - val_loss: 0.3576 - val_acc: 0.8977\n",
      "Epoch 740/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1804 - acc: 0.9333 - val_loss: 0.3403 - val_acc: 0.9063\n",
      "Epoch 741/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1769 - acc: 0.9385 - val_loss: 0.3566 - val_acc: 0.9026\n",
      "Epoch 742/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1688 - acc: 0.9445 - val_loss: 0.3695 - val_acc: 0.8953\n",
      "Epoch 743/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.1780 - acc: 0.9403 - val_loss: 0.3774 - val_acc: 0.8898\n",
      "Epoch 744/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1819 - acc: 0.9315 - val_loss: 0.3439 - val_acc: 0.9057\n",
      "Epoch 745/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.1788 - acc: 0.9363 - val_loss: 0.3635 - val_acc: 0.8983\n",
      "Epoch 746/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 0.1764 - acc: 0.9367 - val_loss: 0.3530 - val_acc: 0.9057\n",
      "Epoch 747/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1687 - acc: 0.9472 - val_loss: 0.3494 - val_acc: 0.9088\n",
      "Epoch 748/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1817 - acc: 0.9388 - val_loss: 0.3845 - val_acc: 0.8892\n",
      "Epoch 749/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1781 - acc: 0.9415 - val_loss: 0.4049 - val_acc: 0.8739\n",
      "Epoch 750/1000\n",
      "3315/3315 [==============================] - 2s 472us/step - loss: 0.1700 - acc: 0.9409 - val_loss: 0.3678 - val_acc: 0.8983\n",
      "Epoch 751/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1712 - acc: 0.9388 - val_loss: 0.3597 - val_acc: 0.9008\n",
      "Epoch 752/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1799 - acc: 0.9342 - val_loss: 0.3812 - val_acc: 0.8934\n",
      "Epoch 753/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1853 - acc: 0.9409 - val_loss: 0.3513 - val_acc: 0.9002\n",
      "Epoch 754/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.1816 - acc: 0.9403 - val_loss: 0.3437 - val_acc: 0.8965\n",
      "Epoch 755/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.1758 - acc: 0.9376 - val_loss: 0.3471 - val_acc: 0.9014\n",
      "Epoch 756/1000\n",
      "3315/3315 [==============================] - 2s 508us/step - loss: 0.1722 - acc: 0.9418 - val_loss: 0.3412 - val_acc: 0.9057\n",
      "Epoch 757/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 0.1702 - acc: 0.9412 - val_loss: 0.3680 - val_acc: 0.8983\n",
      "Epoch 758/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1676 - acc: 0.9418 - val_loss: 0.3696 - val_acc: 0.8922\n",
      "Epoch 759/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1657 - acc: 0.9463 - val_loss: 0.3402 - val_acc: 0.9100\n",
      "Epoch 760/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.1700 - acc: 0.9433 - val_loss: 0.3491 - val_acc: 0.9014\n",
      "Epoch 761/1000\n",
      "3315/3315 [==============================] - 2s 510us/step - loss: 0.1652 - acc: 0.9409 - val_loss: 0.3730 - val_acc: 0.8977\n",
      "Epoch 762/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1766 - acc: 0.9397 - val_loss: 0.3379 - val_acc: 0.9075\n",
      "Epoch 763/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1640 - acc: 0.9490 - val_loss: 0.3637 - val_acc: 0.8879\n",
      "Epoch 764/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1833 - acc: 0.9354 - val_loss: 0.3519 - val_acc: 0.8928\n",
      "Epoch 765/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1684 - acc: 0.9466 - val_loss: 0.3535 - val_acc: 0.9008\n",
      "Epoch 766/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1682 - acc: 0.9427 - val_loss: 0.3641 - val_acc: 0.8996\n",
      "Epoch 767/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.1787 - acc: 0.9424 - val_loss: 0.3396 - val_acc: 0.9081\n",
      "Epoch 768/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.1677 - acc: 0.9460 - val_loss: 0.3517 - val_acc: 0.9045\n",
      "Epoch 769/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.1750 - acc: 0.9388 - val_loss: 0.3558 - val_acc: 0.8977\n",
      "Epoch 770/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.1713 - acc: 0.9439 - val_loss: 0.3547 - val_acc: 0.9014\n",
      "Epoch 771/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.1638 - acc: 0.9457 - val_loss: 0.3610 - val_acc: 0.9069\n",
      "Epoch 772/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.1699 - acc: 0.9403 - val_loss: 0.3509 - val_acc: 0.9026\n",
      "Epoch 773/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1753 - acc: 0.9406 - val_loss: 0.3633 - val_acc: 0.9014\n",
      "Epoch 774/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1702 - acc: 0.9454 - val_loss: 0.3597 - val_acc: 0.9032\n",
      "Epoch 775/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1543 - acc: 0.9499 - val_loss: 0.3668 - val_acc: 0.9002\n",
      "Epoch 776/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1649 - acc: 0.9412 - val_loss: 0.3545 - val_acc: 0.8977\n",
      "Epoch 777/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.1540 - acc: 0.9502 - val_loss: 0.3515 - val_acc: 0.9088\n",
      "Epoch 778/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.1607 - acc: 0.9472 - val_loss: 0.3423 - val_acc: 0.9088\n",
      "Epoch 779/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1536 - acc: 0.9466 - val_loss: 0.3564 - val_acc: 0.9057\n",
      "Epoch 780/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1588 - acc: 0.9466 - val_loss: 0.3571 - val_acc: 0.9008\n",
      "Epoch 781/1000\n",
      "3315/3315 [==============================] - 2s 479us/step - loss: 0.1499 - acc: 0.9487 - val_loss: 0.3478 - val_acc: 0.9002\n",
      "Epoch 782/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.1646 - acc: 0.9481 - val_loss: 0.3522 - val_acc: 0.9094\n",
      "Epoch 783/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1564 - acc: 0.9451 - val_loss: 0.3509 - val_acc: 0.9026\n",
      "Epoch 784/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1668 - acc: 0.9394 - val_loss: 0.3539 - val_acc: 0.9039\n",
      "Epoch 785/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1655 - acc: 0.9442 - val_loss: 0.3398 - val_acc: 0.9051\n",
      "Epoch 786/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.1631 - acc: 0.9451 - val_loss: 0.3674 - val_acc: 0.8996\n",
      "Epoch 787/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.1558 - acc: 0.9463 - val_loss: 0.3420 - val_acc: 0.9039\n",
      "Epoch 788/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1657 - acc: 0.9460 - val_loss: 0.3463 - val_acc: 0.9045\n",
      "Epoch 789/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1487 - acc: 0.9478 - val_loss: 0.3680 - val_acc: 0.8977\n",
      "Epoch 790/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.1582 - acc: 0.9472 - val_loss: 0.3441 - val_acc: 0.9100\n",
      "Epoch 791/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1678 - acc: 0.9436 - val_loss: 0.3374 - val_acc: 0.9088\n",
      "Epoch 792/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1623 - acc: 0.9427 - val_loss: 0.3401 - val_acc: 0.9051\n",
      "Epoch 793/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1557 - acc: 0.9469 - val_loss: 0.3468 - val_acc: 0.9026\n",
      "Epoch 794/1000\n",
      "3315/3315 [==============================] - 2s 517us/step - loss: 0.1618 - acc: 0.9478 - val_loss: 0.3476 - val_acc: 0.9088\n",
      "Epoch 795/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.1649 - acc: 0.9418 - val_loss: 0.3631 - val_acc: 0.8965\n",
      "Epoch 796/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1689 - acc: 0.9457 - val_loss: 0.3569 - val_acc: 0.9026\n",
      "Epoch 797/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1675 - acc: 0.9421 - val_loss: 0.3532 - val_acc: 0.9075\n",
      "Epoch 798/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1574 - acc: 0.9469 - val_loss: 0.3626 - val_acc: 0.9039\n",
      "Epoch 799/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.1641 - acc: 0.9466 - val_loss: 0.3469 - val_acc: 0.9063\n",
      "Epoch 800/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.1605 - acc: 0.9451 - val_loss: 0.3479 - val_acc: 0.9032\n",
      "Epoch 801/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.1694 - acc: 0.9397 - val_loss: 0.3382 - val_acc: 0.9143\n",
      "Epoch 802/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.1557 - acc: 0.9439 - val_loss: 0.3466 - val_acc: 0.9039\n",
      "Epoch 803/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1564 - acc: 0.9469 - val_loss: 0.3464 - val_acc: 0.9026\n",
      "Epoch 804/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.1552 - acc: 0.9487 - val_loss: 0.3408 - val_acc: 0.9137\n",
      "Epoch 805/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1575 - acc: 0.9472 - val_loss: 0.3463 - val_acc: 0.9039\n",
      "Epoch 806/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1500 - acc: 0.9472 - val_loss: 0.3513 - val_acc: 0.8996\n",
      "Epoch 807/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1565 - acc: 0.9487 - val_loss: 0.3432 - val_acc: 0.9039\n",
      "Epoch 808/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1666 - acc: 0.9439 - val_loss: 0.3544 - val_acc: 0.9081\n",
      "Epoch 809/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.1649 - acc: 0.9433 - val_loss: 0.3406 - val_acc: 0.9057\n",
      "Epoch 810/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1525 - acc: 0.9508 - val_loss: 0.3397 - val_acc: 0.9057\n",
      "Epoch 811/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1554 - acc: 0.9475 - val_loss: 0.3549 - val_acc: 0.9020\n",
      "Epoch 812/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1570 - acc: 0.9430 - val_loss: 0.3532 - val_acc: 0.8990\n",
      "Epoch 813/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1626 - acc: 0.9463 - val_loss: 0.3418 - val_acc: 0.8971\n",
      "Epoch 814/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1499 - acc: 0.9472 - val_loss: 0.3491 - val_acc: 0.9124\n",
      "Epoch 815/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1588 - acc: 0.9445 - val_loss: 0.3617 - val_acc: 0.8910\n",
      "Epoch 816/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.1548 - acc: 0.9460 - val_loss: 0.3648 - val_acc: 0.9020\n",
      "Epoch 817/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1557 - acc: 0.9484 - val_loss: 0.3507 - val_acc: 0.9057\n",
      "Epoch 818/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.1692 - acc: 0.9460 - val_loss: 0.3548 - val_acc: 0.9008\n",
      "Epoch 819/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.1550 - acc: 0.9457 - val_loss: 0.3353 - val_acc: 0.9094\n",
      "Epoch 820/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1623 - acc: 0.9436 - val_loss: 0.3215 - val_acc: 0.9106\n",
      "Epoch 821/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.1619 - acc: 0.9457 - val_loss: 0.3397 - val_acc: 0.9039\n",
      "Epoch 822/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1580 - acc: 0.9487 - val_loss: 0.3395 - val_acc: 0.9002\n",
      "Epoch 823/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.1553 - acc: 0.9481 - val_loss: 0.3351 - val_acc: 0.9100\n",
      "Epoch 824/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1666 - acc: 0.9418 - val_loss: 0.3527 - val_acc: 0.9020\n",
      "Epoch 825/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.1463 - acc: 0.9508 - val_loss: 0.3436 - val_acc: 0.9002\n",
      "Epoch 826/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1512 - acc: 0.9529 - val_loss: 0.3285 - val_acc: 0.9075\n",
      "Epoch 827/1000\n",
      "3315/3315 [==============================] - 2s 515us/step - loss: 0.1554 - acc: 0.9466 - val_loss: 0.3500 - val_acc: 0.9051\n",
      "Epoch 828/1000\n",
      "3315/3315 [==============================] - 2s 515us/step - loss: 0.1546 - acc: 0.9478 - val_loss: 0.3258 - val_acc: 0.9149\n",
      "Epoch 829/1000\n",
      "3315/3315 [==============================] - 2s 514us/step - loss: 0.1430 - acc: 0.9517 - val_loss: 0.3411 - val_acc: 0.9057\n",
      "Epoch 830/1000\n",
      "3315/3315 [==============================] - 2s 521us/step - loss: 0.1572 - acc: 0.9484 - val_loss: 0.3438 - val_acc: 0.9063\n",
      "Epoch 831/1000\n",
      "3315/3315 [==============================] - 2s 510us/step - loss: 0.1442 - acc: 0.9460 - val_loss: 0.3626 - val_acc: 0.8971\n",
      "Epoch 832/1000\n",
      "3315/3315 [==============================] - 2s 510us/step - loss: 0.1470 - acc: 0.9551 - val_loss: 0.3451 - val_acc: 0.9039\n",
      "Epoch 833/1000\n",
      "3315/3315 [==============================] - 2s 508us/step - loss: 0.1564 - acc: 0.9454 - val_loss: 0.3451 - val_acc: 0.9032\n",
      "Epoch 834/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1496 - acc: 0.9499 - val_loss: 0.3512 - val_acc: 0.9014\n",
      "Epoch 835/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1593 - acc: 0.9445 - val_loss: 0.3572 - val_acc: 0.9014\n",
      "Epoch 836/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.1462 - acc: 0.9526 - val_loss: 0.3674 - val_acc: 0.9081\n",
      "Epoch 837/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 0.1634 - acc: 0.9478 - val_loss: 0.3622 - val_acc: 0.9032\n",
      "Epoch 838/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 0.1587 - acc: 0.9469 - val_loss: 0.3459 - val_acc: 0.9032\n",
      "Epoch 839/1000\n",
      "3315/3315 [==============================] - 2s 508us/step - loss: 0.1521 - acc: 0.9484 - val_loss: 0.3400 - val_acc: 0.9069\n",
      "Epoch 840/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.1369 - acc: 0.9508 - val_loss: 0.3319 - val_acc: 0.9106\n",
      "Epoch 841/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1451 - acc: 0.9514 - val_loss: 0.3321 - val_acc: 0.9063\n",
      "Epoch 842/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1524 - acc: 0.9511 - val_loss: 0.3492 - val_acc: 0.9063\n",
      "Epoch 843/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1387 - acc: 0.9490 - val_loss: 0.3380 - val_acc: 0.9143\n",
      "Epoch 844/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1436 - acc: 0.9523 - val_loss: 0.3515 - val_acc: 0.9026\n",
      "Epoch 845/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.1530 - acc: 0.9529 - val_loss: 0.3410 - val_acc: 0.9088\n",
      "Epoch 846/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1501 - acc: 0.9481 - val_loss: 0.3580 - val_acc: 0.9014\n",
      "Epoch 847/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1360 - acc: 0.9551 - val_loss: 0.3317 - val_acc: 0.9088\n",
      "Epoch 848/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1466 - acc: 0.9490 - val_loss: 0.3565 - val_acc: 0.9051\n",
      "Epoch 849/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1470 - acc: 0.9514 - val_loss: 0.3536 - val_acc: 0.9057\n",
      "Epoch 850/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.1464 - acc: 0.9511 - val_loss: 0.3574 - val_acc: 0.8990\n",
      "Epoch 851/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1537 - acc: 0.9454 - val_loss: 0.3461 - val_acc: 0.8965\n",
      "Epoch 852/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1554 - acc: 0.9454 - val_loss: 0.3541 - val_acc: 0.9112\n",
      "Epoch 853/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.1417 - acc: 0.9544 - val_loss: 0.3671 - val_acc: 0.9051\n",
      "Epoch 854/1000\n",
      "3315/3315 [==============================] - 2s 514us/step - loss: 0.1428 - acc: 0.9505 - val_loss: 0.3541 - val_acc: 0.9081\n",
      "Epoch 855/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.1521 - acc: 0.9460 - val_loss: 0.3444 - val_acc: 0.9063\n",
      "Epoch 856/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1559 - acc: 0.9484 - val_loss: 0.3688 - val_acc: 0.9088\n",
      "Epoch 857/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1601 - acc: 0.9493 - val_loss: 0.3523 - val_acc: 0.9124\n",
      "Epoch 858/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1514 - acc: 0.9490 - val_loss: 0.3297 - val_acc: 0.9143\n",
      "Epoch 859/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.1544 - acc: 0.9433 - val_loss: 0.3282 - val_acc: 0.9173\n",
      "Epoch 860/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.1460 - acc: 0.9502 - val_loss: 0.3524 - val_acc: 0.9088\n",
      "Epoch 861/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.1356 - acc: 0.9514 - val_loss: 0.3565 - val_acc: 0.9057\n",
      "Epoch 862/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.1503 - acc: 0.9505 - val_loss: 0.3472 - val_acc: 0.9051\n",
      "Epoch 863/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.1441 - acc: 0.9514 - val_loss: 0.3462 - val_acc: 0.9118\n",
      "Epoch 864/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1633 - acc: 0.9445 - val_loss: 0.3486 - val_acc: 0.9032\n",
      "Epoch 865/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.1476 - acc: 0.9481 - val_loss: 0.3323 - val_acc: 0.9124\n",
      "Epoch 866/1000\n",
      "3315/3315 [==============================] - 2s 573us/step - loss: 0.1422 - acc: 0.9529 - val_loss: 0.3445 - val_acc: 0.9063\n",
      "Epoch 867/1000\n",
      "3315/3315 [==============================] - 2s 578us/step - loss: 0.1481 - acc: 0.9481 - val_loss: 0.3487 - val_acc: 0.9051\n",
      "Epoch 868/1000\n",
      "3315/3315 [==============================] - 2s 556us/step - loss: 0.1542 - acc: 0.9457 - val_loss: 0.3551 - val_acc: 0.9069\n",
      "Epoch 869/1000\n",
      "3315/3315 [==============================] - 2s 546us/step - loss: 0.1513 - acc: 0.9478 - val_loss: 0.3690 - val_acc: 0.9020\n",
      "Epoch 870/1000\n",
      "3315/3315 [==============================] - 2s 541us/step - loss: 0.1510 - acc: 0.9514 - val_loss: 0.3593 - val_acc: 0.8983\n",
      "Epoch 871/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.1458 - acc: 0.9484 - val_loss: 0.3427 - val_acc: 0.9143\n",
      "Epoch 872/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.1586 - acc: 0.9511 - val_loss: 0.3636 - val_acc: 0.8990\n",
      "Epoch 873/1000\n",
      "3315/3315 [==============================] - 2s 479us/step - loss: 0.1335 - acc: 0.9535 - val_loss: 0.3478 - val_acc: 0.9143\n",
      "Epoch 874/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.1411 - acc: 0.9517 - val_loss: 0.3539 - val_acc: 0.9002\n",
      "Epoch 875/1000\n",
      "3315/3315 [==============================] - 2s 479us/step - loss: 0.1475 - acc: 0.9469 - val_loss: 0.3411 - val_acc: 0.9106\n",
      "Epoch 876/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.1521 - acc: 0.9487 - val_loss: 0.3455 - val_acc: 0.9014\n",
      "Epoch 877/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1300 - acc: 0.9532 - val_loss: 0.3488 - val_acc: 0.9137\n",
      "Epoch 878/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.1445 - acc: 0.9499 - val_loss: 0.3429 - val_acc: 0.9081\n",
      "Epoch 879/1000\n",
      "3315/3315 [==============================] - 2s 472us/step - loss: 0.1407 - acc: 0.9535 - val_loss: 0.3572 - val_acc: 0.9032\n",
      "Epoch 880/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.1588 - acc: 0.9493 - val_loss: 0.3371 - val_acc: 0.9088\n",
      "Epoch 881/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.1377 - acc: 0.9532 - val_loss: 0.3474 - val_acc: 0.9118\n",
      "Epoch 882/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.1446 - acc: 0.9514 - val_loss: 0.3351 - val_acc: 0.9112\n",
      "Epoch 883/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1422 - acc: 0.9523 - val_loss: 0.3465 - val_acc: 0.9118\n",
      "Epoch 884/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.1493 - acc: 0.9508 - val_loss: 0.3407 - val_acc: 0.9094\n",
      "Epoch 885/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1442 - acc: 0.9511 - val_loss: 0.3575 - val_acc: 0.9026\n",
      "Epoch 886/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.1502 - acc: 0.9484 - val_loss: 0.3405 - val_acc: 0.9161\n",
      "Epoch 887/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1507 - acc: 0.9484 - val_loss: 0.3415 - val_acc: 0.9069\n",
      "Epoch 888/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.1494 - acc: 0.9475 - val_loss: 0.3639 - val_acc: 0.9069\n",
      "Epoch 889/1000\n",
      "3315/3315 [==============================] - 2s 553us/step - loss: 0.1419 - acc: 0.9548 - val_loss: 0.3383 - val_acc: 0.9130\n",
      "Epoch 890/1000\n",
      "3315/3315 [==============================] - 2s 555us/step - loss: 0.1488 - acc: 0.9523 - val_loss: 0.3465 - val_acc: 0.9137\n",
      "Epoch 891/1000\n",
      "3315/3315 [==============================] - 2s 558us/step - loss: 0.1441 - acc: 0.9517 - val_loss: 0.3344 - val_acc: 0.9137\n",
      "Epoch 892/1000\n",
      "3315/3315 [==============================] - 2s 498us/step - loss: 0.1294 - acc: 0.9557 - val_loss: 0.3742 - val_acc: 0.8971\n",
      "Epoch 893/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.1354 - acc: 0.9575 - val_loss: 0.3477 - val_acc: 0.9112\n",
      "Epoch 894/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.1348 - acc: 0.9514 - val_loss: 0.3514 - val_acc: 0.9088\n",
      "Epoch 895/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1464 - acc: 0.9478 - val_loss: 0.3395 - val_acc: 0.9100\n",
      "Epoch 896/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1461 - acc: 0.9514 - val_loss: 0.3417 - val_acc: 0.9149\n",
      "Epoch 897/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1369 - acc: 0.9511 - val_loss: 0.3380 - val_acc: 0.9179\n",
      "Epoch 898/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.1281 - acc: 0.9578 - val_loss: 0.3311 - val_acc: 0.9155\n",
      "Epoch 899/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1318 - acc: 0.9538 - val_loss: 0.3264 - val_acc: 0.9124\n",
      "Epoch 900/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.1407 - acc: 0.9505 - val_loss: 0.3554 - val_acc: 0.9112\n",
      "Epoch 901/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1357 - acc: 0.9557 - val_loss: 0.3428 - val_acc: 0.9155\n",
      "Epoch 902/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1433 - acc: 0.9532 - val_loss: 0.3302 - val_acc: 0.9051\n",
      "Epoch 903/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.1269 - acc: 0.9584 - val_loss: 0.3487 - val_acc: 0.9100\n",
      "Epoch 904/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1325 - acc: 0.9548 - val_loss: 0.3307 - val_acc: 0.9210\n",
      "Epoch 905/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1370 - acc: 0.9538 - val_loss: 0.3329 - val_acc: 0.9106\n",
      "Epoch 906/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.1449 - acc: 0.9526 - val_loss: 0.3610 - val_acc: 0.9032\n",
      "Epoch 907/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1383 - acc: 0.9563 - val_loss: 0.3436 - val_acc: 0.9155\n",
      "Epoch 908/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.1475 - acc: 0.9493 - val_loss: 0.3598 - val_acc: 0.9057\n",
      "Epoch 909/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.1463 - acc: 0.9548 - val_loss: 0.3599 - val_acc: 0.9026\n",
      "Epoch 910/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1367 - acc: 0.9590 - val_loss: 0.3335 - val_acc: 0.9204\n",
      "Epoch 911/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.1371 - acc: 0.9514 - val_loss: 0.3813 - val_acc: 0.8990\n",
      "Epoch 912/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.1304 - acc: 0.9572 - val_loss: 0.3550 - val_acc: 0.9057\n",
      "Epoch 913/1000\n",
      "3315/3315 [==============================] - 2s 484us/step - loss: 0.1438 - acc: 0.9484 - val_loss: 0.3333 - val_acc: 0.9100\n",
      "Epoch 914/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1435 - acc: 0.9538 - val_loss: 0.3427 - val_acc: 0.9112\n",
      "Epoch 915/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.1305 - acc: 0.9575 - val_loss: 0.3409 - val_acc: 0.9081\n",
      "Epoch 916/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1305 - acc: 0.9526 - val_loss: 0.3469 - val_acc: 0.9045\n",
      "Epoch 917/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1383 - acc: 0.9554 - val_loss: 0.3419 - val_acc: 0.9063\n",
      "Epoch 918/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.1212 - acc: 0.9596 - val_loss: 0.3474 - val_acc: 0.9118\n",
      "Epoch 919/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.1336 - acc: 0.9544 - val_loss: 0.3430 - val_acc: 0.9143\n",
      "Epoch 920/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1411 - acc: 0.9532 - val_loss: 0.3539 - val_acc: 0.9051\n",
      "Epoch 921/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1282 - acc: 0.9587 - val_loss: 0.3582 - val_acc: 0.9118\n",
      "Epoch 922/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1433 - acc: 0.9520 - val_loss: 0.3434 - val_acc: 0.9094\n",
      "Epoch 923/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.1337 - acc: 0.9505 - val_loss: 0.3461 - val_acc: 0.9045\n",
      "Epoch 924/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.1369 - acc: 0.9544 - val_loss: 0.3359 - val_acc: 0.9143\n",
      "Epoch 925/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1460 - acc: 0.9487 - val_loss: 0.3327 - val_acc: 0.9088\n",
      "Epoch 926/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.1325 - acc: 0.9569 - val_loss: 0.3493 - val_acc: 0.9130\n",
      "Epoch 927/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.1277 - acc: 0.9584 - val_loss: 0.3492 - val_acc: 0.9063\n",
      "Epoch 928/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.1135 - acc: 0.9638 - val_loss: 0.3515 - val_acc: 0.9032\n",
      "Epoch 929/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.1607 - acc: 0.9472 - val_loss: 0.3469 - val_acc: 0.9124\n",
      "Epoch 930/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.1344 - acc: 0.9508 - val_loss: 0.3502 - val_acc: 0.9088\n",
      "Epoch 931/1000\n",
      "3315/3315 [==============================] - 2s 476us/step - loss: 0.1376 - acc: 0.9554 - val_loss: 0.3561 - val_acc: 0.9106\n",
      "Epoch 932/1000\n",
      "3315/3315 [==============================] - 2s 492us/step - loss: 0.1436 - acc: 0.9541 - val_loss: 0.3531 - val_acc: 0.9124\n",
      "Epoch 933/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 0.1355 - acc: 0.9557 - val_loss: 0.3349 - val_acc: 0.9167\n",
      "Epoch 934/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1328 - acc: 0.9541 - val_loss: 0.3613 - val_acc: 0.9081\n",
      "Epoch 935/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1371 - acc: 0.9575 - val_loss: 0.3719 - val_acc: 0.9020\n",
      "Epoch 936/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.1393 - acc: 0.9541 - val_loss: 0.3540 - val_acc: 0.9026\n",
      "Epoch 937/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1370 - acc: 0.9554 - val_loss: 0.3505 - val_acc: 0.9137\n",
      "Epoch 938/1000\n",
      "3315/3315 [==============================] - 2s 508us/step - loss: 0.1260 - acc: 0.9608 - val_loss: 0.3567 - val_acc: 0.9100\n",
      "Epoch 939/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 0.1389 - acc: 0.9535 - val_loss: 0.3747 - val_acc: 0.8990\n",
      "Epoch 940/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1316 - acc: 0.9529 - val_loss: 0.3557 - val_acc: 0.9075\n",
      "Epoch 941/1000\n",
      "3315/3315 [==============================] - 2s 477us/step - loss: 0.1354 - acc: 0.9548 - val_loss: 0.3593 - val_acc: 0.9081\n",
      "Epoch 942/1000\n",
      "3315/3315 [==============================] - 2s 485us/step - loss: 0.1337 - acc: 0.9566 - val_loss: 0.3536 - val_acc: 0.9075\n",
      "Epoch 943/1000\n",
      "3315/3315 [==============================] - 2s 479us/step - loss: 0.1324 - acc: 0.9551 - val_loss: 0.3556 - val_acc: 0.9106\n",
      "Epoch 944/1000\n",
      "3315/3315 [==============================] - 2s 479us/step - loss: 0.1375 - acc: 0.9557 - val_loss: 0.3501 - val_acc: 0.9137\n",
      "Epoch 945/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.1329 - acc: 0.9569 - val_loss: 0.3614 - val_acc: 0.9081\n",
      "Epoch 946/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.1325 - acc: 0.9548 - val_loss: 0.3388 - val_acc: 0.9130\n",
      "Epoch 947/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.1274 - acc: 0.9532 - val_loss: 0.3624 - val_acc: 0.9051\n",
      "Epoch 948/1000\n",
      "3315/3315 [==============================] - 2s 480us/step - loss: 0.1443 - acc: 0.9538 - val_loss: 0.3560 - val_acc: 0.9094\n",
      "Epoch 949/1000\n",
      "3315/3315 [==============================] - 2s 483us/step - loss: 0.1249 - acc: 0.9557 - val_loss: 0.3550 - val_acc: 0.9112\n",
      "Epoch 950/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1354 - acc: 0.9535 - val_loss: 0.3380 - val_acc: 0.9094\n",
      "Epoch 951/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.1279 - acc: 0.9584 - val_loss: 0.3417 - val_acc: 0.9081\n",
      "Epoch 952/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.1261 - acc: 0.9572 - val_loss: 0.3498 - val_acc: 0.9149\n",
      "Epoch 953/1000\n",
      "3315/3315 [==============================] - 2s 477us/step - loss: 0.1233 - acc: 0.9581 - val_loss: 0.3472 - val_acc: 0.9094\n",
      "Epoch 954/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.1350 - acc: 0.9517 - val_loss: 0.3238 - val_acc: 0.9253\n",
      "Epoch 955/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1318 - acc: 0.9532 - val_loss: 0.3448 - val_acc: 0.9063\n",
      "Epoch 956/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.1330 - acc: 0.9563 - val_loss: 0.3197 - val_acc: 0.9222\n",
      "Epoch 957/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1430 - acc: 0.9490 - val_loss: 0.3373 - val_acc: 0.9179\n",
      "Epoch 958/1000\n",
      "3315/3315 [==============================] - 2s 497us/step - loss: 0.1230 - acc: 0.9596 - val_loss: 0.3369 - val_acc: 0.9161\n",
      "Epoch 959/1000\n",
      "3315/3315 [==============================] - 2s 490us/step - loss: 0.1228 - acc: 0.9581 - val_loss: 0.3437 - val_acc: 0.9149\n",
      "Epoch 960/1000\n",
      "3315/3315 [==============================] - 2s 487us/step - loss: 0.1195 - acc: 0.9593 - val_loss: 0.3376 - val_acc: 0.9063\n",
      "Epoch 961/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.1372 - acc: 0.9520 - val_loss: 0.3401 - val_acc: 0.9124\n",
      "Epoch 962/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1293 - acc: 0.9529 - val_loss: 0.3362 - val_acc: 0.9161\n",
      "Epoch 963/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1312 - acc: 0.9538 - val_loss: 0.3455 - val_acc: 0.9081\n",
      "Epoch 964/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.1313 - acc: 0.9560 - val_loss: 0.3454 - val_acc: 0.9173\n",
      "Epoch 965/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1125 - acc: 0.9638 - val_loss: 0.3739 - val_acc: 0.9075\n",
      "Epoch 966/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 0.1400 - acc: 0.9541 - val_loss: 0.3535 - val_acc: 0.9039\n",
      "Epoch 967/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.1435 - acc: 0.9520 - val_loss: 0.3562 - val_acc: 0.9112\n",
      "Epoch 968/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1278 - acc: 0.9538 - val_loss: 0.3371 - val_acc: 0.9161\n",
      "Epoch 969/1000\n",
      "3315/3315 [==============================] - 2s 500us/step - loss: 0.1276 - acc: 0.9581 - val_loss: 0.3441 - val_acc: 0.9149\n",
      "Epoch 970/1000\n",
      "3315/3315 [==============================] - 2s 511us/step - loss: 0.1295 - acc: 0.9544 - val_loss: 0.3533 - val_acc: 0.9100\n",
      "Epoch 971/1000\n",
      "3315/3315 [==============================] - 2s 507us/step - loss: 0.1360 - acc: 0.9563 - val_loss: 0.3466 - val_acc: 0.9100\n",
      "Epoch 972/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.1192 - acc: 0.9605 - val_loss: 0.3340 - val_acc: 0.9137\n",
      "Epoch 973/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1375 - acc: 0.9517 - val_loss: 0.3668 - val_acc: 0.9057\n",
      "Epoch 974/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1252 - acc: 0.9581 - val_loss: 0.3401 - val_acc: 0.9161\n",
      "Epoch 975/1000\n",
      "3315/3315 [==============================] - 2s 515us/step - loss: 0.1291 - acc: 0.9563 - val_loss: 0.3515 - val_acc: 0.9094\n",
      "Epoch 976/1000\n",
      "3315/3315 [==============================] - 2s 505us/step - loss: 0.1275 - acc: 0.9572 - val_loss: 0.3433 - val_acc: 0.9118\n",
      "Epoch 977/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1291 - acc: 0.9532 - val_loss: 0.3318 - val_acc: 0.9179\n",
      "Epoch 978/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1252 - acc: 0.9563 - val_loss: 0.3447 - val_acc: 0.9112\n",
      "Epoch 979/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1369 - acc: 0.9538 - val_loss: 0.3229 - val_acc: 0.9173\n",
      "Epoch 980/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1259 - acc: 0.9557 - val_loss: 0.3470 - val_acc: 0.9106\n",
      "Epoch 981/1000\n",
      "3315/3315 [==============================] - 2s 482us/step - loss: 0.1309 - acc: 0.9566 - val_loss: 0.3599 - val_acc: 0.9094\n",
      "Epoch 982/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1079 - acc: 0.9656 - val_loss: 0.3396 - val_acc: 0.9124\n",
      "Epoch 983/1000\n",
      "3315/3315 [==============================] - 2s 486us/step - loss: 0.1217 - acc: 0.9578 - val_loss: 0.3666 - val_acc: 0.9063\n",
      "Epoch 984/1000\n",
      "3315/3315 [==============================] - 2s 489us/step - loss: 0.1374 - acc: 0.9496 - val_loss: 0.3488 - val_acc: 0.9137\n",
      "Epoch 985/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1334 - acc: 0.9554 - val_loss: 0.3592 - val_acc: 0.9118\n",
      "Epoch 986/1000\n",
      "3315/3315 [==============================] - 2s 493us/step - loss: 0.1255 - acc: 0.9566 - val_loss: 0.3366 - val_acc: 0.9088\n",
      "Epoch 987/1000\n",
      "3315/3315 [==============================] - 2s 495us/step - loss: 0.1155 - acc: 0.9590 - val_loss: 0.3608 - val_acc: 0.9069\n",
      "Epoch 988/1000\n",
      "3315/3315 [==============================] - 2s 491us/step - loss: 0.1342 - acc: 0.9554 - val_loss: 0.3469 - val_acc: 0.9100\n",
      "Epoch 989/1000\n",
      "3315/3315 [==============================] - 2s 481us/step - loss: 0.1304 - acc: 0.9538 - val_loss: 0.3348 - val_acc: 0.9179\n",
      "Epoch 990/1000\n",
      "3315/3315 [==============================] - 2s 494us/step - loss: 0.1107 - acc: 0.9638 - val_loss: 0.3483 - val_acc: 0.9124\n",
      "Epoch 991/1000\n",
      "3315/3315 [==============================] - 2s 504us/step - loss: 0.1339 - acc: 0.9554 - val_loss: 0.3617 - val_acc: 0.9094\n",
      "Epoch 992/1000\n",
      "3315/3315 [==============================] - 2s 501us/step - loss: 0.1268 - acc: 0.9554 - val_loss: 0.3442 - val_acc: 0.9161\n",
      "Epoch 993/1000\n",
      "3315/3315 [==============================] - 2s 502us/step - loss: 0.1132 - acc: 0.9599 - val_loss: 0.3340 - val_acc: 0.9063\n",
      "Epoch 994/1000\n",
      "3315/3315 [==============================] - 2s 503us/step - loss: 0.1288 - acc: 0.9557 - val_loss: 0.3234 - val_acc: 0.9222\n",
      "Epoch 995/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1137 - acc: 0.9638 - val_loss: 0.3372 - val_acc: 0.9186\n",
      "Epoch 996/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.1347 - acc: 0.9532 - val_loss: 0.3578 - val_acc: 0.9106\n",
      "Epoch 997/1000\n",
      "3315/3315 [==============================] - 2s 496us/step - loss: 0.1281 - acc: 0.9569 - val_loss: 0.3536 - val_acc: 0.9137\n",
      "Epoch 998/1000\n",
      "3315/3315 [==============================] - 2s 499us/step - loss: 0.1297 - acc: 0.9578 - val_loss: 0.3184 - val_acc: 0.9143\n",
      "Epoch 999/1000\n",
      "3315/3315 [==============================] - 2s 506us/step - loss: 0.1217 - acc: 0.9608 - val_loss: 0.3239 - val_acc: 0.9216\n",
      "Epoch 1000/1000\n",
      "3315/3315 [==============================] - 2s 488us/step - loss: 0.1131 - acc: 0.9590 - val_loss: 0.3428 - val_acc: 0.9186\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFytY6LDzgJ0"
   },
   "source": [
    "Let's plot the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "TFz4ClZov9gZ",
    "outputId": "9aa55f52-22d6-4f4d-c6b6-26ddf2f80d0f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vf1W7LgP2DA5"
   },
   "source": [
    "\n",
    "\n",
    "And now let's plot the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "8yyFBt7ASPUe",
    "outputId": "7bc8eb5c-ac78-409a-a5e2-720d250923a6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npBmgR6MZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['acc'])\n",
    "plt.plot(cnnhistory.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gaZONl1mD8XD"
   },
   "source": [
    "Let's now create a classification report to review the f1-score of the model per class.\n",
    "To do so, we have to:\n",
    "- Create a variable predictions that will contain the model.predict_classes outcome\n",
    "- Convert our y_test (array of strings with our classes) to an array of int called new_Ytest, otherwise it will not be comparable to the predictions by the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EO25uIL-9vqx"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(x_testcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1i06grlBBSrn",
    "outputId": "5d41fbd6-2b99-419a-e93a-37aad390aeb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, ..., 1, 4, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HUHshx93CM_6",
    "outputId": "e7d37848-5f08-4c9f-b703-b98bf59fb6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, ..., 1, 4, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMxojpvWCxOs"
   },
   "outputs": [],
   "source": [
    "new_Ytest = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W07EQaC8DE6i",
    "outputId": "fbde01c6-4edc-45e4-b39b-f620c34db96e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, ..., 1, 4, 0])"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FW2XHdTtEedk"
   },
   "source": [
    "Okay, now we can display the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "IfVSRmMu96rC",
    "outputId": "29ba887e-73f7-4f4e-e732-d0c1c8f301bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       134\n",
      "           1       0.97      0.90      0.93       251\n",
      "           2       0.92      0.91      0.91       242\n",
      "           3       0.86      0.89      0.88       271\n",
      "           4       0.96      0.96      0.96       253\n",
      "           5       0.94      0.90      0.92       239\n",
      "           6       0.85      0.95      0.90       127\n",
      "           7       0.88      0.91      0.90       116\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1633\n",
      "   macro avg       0.91      0.92      0.92      1633\n",
      "weighted avg       0.92      0.92      0.92      1633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(new_Ytest, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hu1S5IowfSDG"
   },
   "source": [
    "And now, the confusion matrix: it will show us the misclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "fdy09SCEd7Cl",
    "outputId": "f8d72fcc-819d-457e-f458-b0dd41ea6616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[128   2   0   3   0   0   1   0]\n",
      " [  2 226   7   8   0   0   8   0]\n",
      " [  4   1 220   4   5   6   0   2]\n",
      " [  2   2   2 241   3   5   7   9]\n",
      " [  2   0   2   2 244   0   1   2]\n",
      " [  1   0   2  19   0 214   2   1]\n",
      " [  0   0   2   2   2   0 121   0]\n",
      " [  0   2   4   0   0   2   2 106]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(new_Ytest, predictions)\n",
    "print (matrix)\n",
    "\n",
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_ySPOyHxkZ3"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f5kRmoD-sdHj",
    "outputId": "a3674fa2-2288-4f5b-fc3a-e50b4ab40018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNUiznKNwUtJ"
   },
   "source": [
    "# Reloading the model to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "T4oAv6Kx8RBE",
    "outputId": "aff2e485-12a4-4bc9-ceac-d4de159e6a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FHtPzc0Y8hfZ"
   },
   "source": [
    "# Checking the accuracy of the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qUi-Zjuf8hDB",
    "outputId": "489d4963-c033-48fb-8a82-48e870dac521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1633/1633 [==============================] - 0s 141us/step\n",
      "Restored model, accuracy: 91.86%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pXH3y7S9A1N"
   },
   "source": [
    "# Thank you for your attention! To be continued.."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EmotionsRecognition.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
